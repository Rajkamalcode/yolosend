An updated version of the legal document analysis code is provided below.

This version implements the following key changes as requested:

1.  **Removal of RAG:** The dynamic question generation and Retrieval-Augmented Generation (RAG) process, which relied on a FAISS index, has been entirely removed.
2.  **Two-Step Prompting:** The analysis is now a two-step process using distinct, hardcoded prompts.
3.  **First Prompt (Data Extraction):** The initial prompt focuses exclusively on extracting structured data from the legal document into a JSON format. The section that previously asked the AI to generate questions has been removed to streamline its task.
4.  **Second Prompt (Grounded Opinion with Hardcoded Rules):** A new, comprehensive prompt has been created. This prompt takes the JSON data from the first step and analyzes it against a detailed set of hardcoded business rules and questions derived directly from the provided CSV. It then generates a section-by-section analysis and a final, grounded legal opinion.

This new approach replaces the flexible but complex RAG system with a more deterministic and controlled analysis based on predefined company guidelines, ensuring consistent evaluation of legal documents.

```python
from loader import *
st.set_page_config(
        page_title="Legal Document Analysis",
        page_icon="üîç",
        layout="wide",  # This ensures full width
        initial_sidebar_state="collapsed"
    )
import faiss
import copy
from reportlab.lib.pagesizes import letter, landscape
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
import io # Make sure io is imported for BytesIO
import re
import json

# Update the ACCESS_KEYWORDS list with more comprehensive terms
ACCESS_KEYWORDS = [
    'road', 'street', 'path', 'pathway', 'lane', 'highway', 'avenue',
    'boulevard', 'drive', 'alley', 'circle', 'court', 'expressway',
    'freeway', 'terrace', 'place', 'square', 'walk', 'rasta', 'way',
    'access', 'approach', 'entry', 'thoroughfare', 'route','gali','track',
    'thoroughfare', 'passage', 'entryway', 'driveway', 'gate', 'ingress',
    'egress', 'corridor', 'pass', 'bypass', 'artery', 'byway', 'lane',
    'boulevard', 'causeway', 'crossing', 'frontage', 'highroad', 'parkway',
    'promenade', 'walkway', 'alleyway', 'esplanade', 'footpath', 'trail',
    'bridge', 'underpass', 'overpass'
]

# 1. Enhanced Address Extraction (Updated)
ADDRESS_KEYWORDS = [
    "survey no.", "patta no.", "door no.", "village", "district", "street",
    "road", "avenue", "town", "city", "state", "pin code", "situate",
    "location", "property at", "schedule", "boundaries of", "property details",
    "khasra no.","gata no.", "plot no.", "situated at", "address", "location"
]

# (Updated)
COMPONENT_PATTERNS = {
    "survey_no": r"(?:Survey|S\.?[rR]\.?|Plot|Khasra)[\s\.,]*[Nn]o\.?\s*([\w\d/.-]+)",
    "patta_no": r"Patta[\s\.,]*[Nn]o\.?\s*([\w\d/.-]+)",
    "door_no": r"Door[\s\.,]*[Nn]o\.?\s*([\w\d/.-]+)",
    "village": r"Village:\s*([\w\s]+)|([\w\s]+Village)",
    "district": r"District:\s*([\w\s]+)|([\w\s]+District)",
    "state": r"State:\s*([\w\s]+)|([\w\s]+State)",
    "pin_code": r"Pin\s*[Cc]ode:\s*(\d{6})|\b(\d{6})\b",
    "area": r"admeasuring\s*([\d.,]+\s*(?:Sq\.?\s*Yard|Sq\.?\s*[Mm]eter|Sq\.?\s*[Ff]t|Acres?|Hectares?|Cents?|Grounds?))",
    "street": r"Street:\s*([\w\s]+)|([\w\s]+Street)"
}

# (New) Function based on user's specific regex for a full address line
def extract_full_address_line(text):
    """Extracts a complete address line based on the 'Property situated at' pattern."""
    pattern = r"Property situated at (.*?)(?=\n|$)"
    match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
    return match.group(1).strip() if match else None


# 2. Owner Name Extraction (Updated with user's keywords)
def extract_owner_names(text):
    """Extracts owner names using a more comprehensive set of patterns."""
    patterns = [
        # User-suggested keywords
        r"Owner[\s\.,]*Name\(?s?\)?[:\s]*([\w\s.,]+)",
        r"Title[\s\.,]*Owner\(?s?\)?[:\s]*([\w\s.,]+)",
        # Original patterns
        r"Owner\(s\):\s*([\w\s.,]+)",
        r"Name\(s\)[\s\.,]*of[\s\.,]*Owner\(s\)[:\s]*([\w\s.,]+)"
    ]
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return match.group(1).strip()
    return None

# 3. Property Extent Distinction (Updated with user's keywords)
def extract_property_extent(text):
    """Extracts land and built-up area using more varied keywords."""
    # Merged patterns from original and user's suggestion
    land_pattern = r"(?:Land[\s\.,]*Area|Plot[\s\.,]*Area|admeasuring)[\s:]*([\d.,]+\s*(?:Sq\.?\s*Yard|Sq\.?\s*[Mm]eter|Sq\.?\s*[Ff]t|Acres?|Hectares?|Cents?|Grounds?|Ankanams?|Guntas?))"
    built_up_pattern = r"(?:Built[\s-]*up[\s\.,]*Area|Constructed[\s\.,]*Area)[\s:]*([\d.,]+\s*(?:Sq\.?\s*Yard|Sq\.?\s*[Mm]eter|Sq\.?\s*[Ff]t|sft))"

    land_match = re.search(land_pattern, text, re.IGNORECASE)
    built_up_match = re.search(built_up_pattern, text, re.IGNORECASE)

    return {
        "land_area": land_match.group(1).strip() if land_match else "Not Found",
        "built_up_area": built_up_match.group(1).strip() if built_up_match else "Not Found"
    }

# 4. Title History Processing (New)
def process_title_history(title_history_list):
    processed = []
    if not isinstance(title_history_list, list):
        return [] # Return empty list if input is not a list

    for entry in title_history_list:
        if not isinstance(entry, dict):
            continue

        # Skip ONLY explicit Patta issuance (not compound documents like "registered Deed/Patta")
        nature_raw = entry.get("nature_of_transfer", "").lower().strip()
        doc_name_raw = entry.get("document_name", "").lower().strip()
        
        # More precise filtering - skip only if it's explicitly "patta" and nothing else
        is_explicit_patta = (
            nature_raw == "patta" or
            nature_raw == "patta issuance" or
            doc_name_raw == "patta" or
            doc_name_raw.startswith("patta ") or
            doc_name_raw.endswith(" patta") or
            (doc_name_raw.startswith("patta") and not ("deed" in doc_name_raw or "registered" in doc_name_raw))
        )
        
        if is_explicit_patta:
            continue  # Skip only explicit patta documents

        # Ensure proper field names
        executed_by = entry.get("executed_by", entry.get("from", "Unknown"))
        in_favour_of = entry.get("in_favour_of", entry.get("to", "Unknown"))

        # Correct transfer nature
        nature_raw = entry.get("nature_of_transfer", "Transfer")
        nature = str(nature_raw).strip() # Ensure it's a string
        nature_lower = nature.lower()

        # Standardize transfer nature names
        if "deed/patta" in nature_lower or "registered deed/patta" in nature_lower:
            nature = "Deed/Patta"
        elif "will" in nature_lower:
            nature = "Will"
        elif "court decree" in nature_lower:
            nature = "Court Decree"
        elif "sale" in nature_lower:
            nature = "Sale"
        elif "gift" in nature_lower:
            nature = "Gift"
        elif "partition" in nature_lower:
            nature = "Partition"
        # Keep original if no match
        
        processed.append({
            "document_date": entry.get("document_date", ""),
            "document_name": entry.get("document_name", ""),
            "executed_by": executed_by,
            "in_favour_of": in_favour_of,
            "nature_of_transfer": nature,
            "extent_owned": entry.get("extent_owned", "Full Property")
        })
    return processed


# 5. Context Length Optimization (New)
def optimize_context(document_text, max_length=30000):
    """Reduce context length while preserving key sections"""
    if len(document_text) <= max_length:
        return document_text

    # Preserve key sections using headers
    preserved_sections = []
    headers = [
        "PROPERTY ADDRESS", "BOUNDARIES", "OWNER NAME", "PROPERTY EXTENT",
        "TITLE HISTORY", "DOCUMENTS EXAMINED", "MORTGAGE DOCUMENTS",
        "LEGAL OPINION", "CONCLUSION"
    ]

    # Create a regex to find content between headers
    header_pattern_str = '|'.join(map(re.escape, headers))
    # This pattern finds a header and captures everything until next header or end
    pattern = re.compile(fr"(\b(?:{header_pattern_str})\b.*?(?=\n\b(?:{header_pattern_str})\b|\Z))",
                         re.IGNORECASE | re.DOTALL)

    matches = pattern.findall(document_text)
    if matches:
        preserved_sections = [match.strip() for match in matches]

    optimized_text = "\n\n".join(preserved_sections)

    # If key sections are still too long or not found, truncate the original
    if not optimized_text or len(optimized_text) > max_length:
        return document_text[:max_length]

    return optimized_text

# 6. Legal Opinion Extraction (Updated with user's pattern)
def extract_legal_opinion(text):
    """Extracts the legal opinion using a list of common section headers."""
    patterns = [
        # User-suggested pattern (modified for robustness)
        r"Conclusion/ observation, if any\.(.*?)(?=\n\n[A-Z\s]{3,}:|\Z)",
        # Original patterns
        r"LEGAL OPINION(.*?)(?=\n\n[A-Z\s]{3,}:|\Z)",
        r"OPINION(.*?)(?=\n\n[A-Z\s]{3,}:|\Z)",
        r"REMARKS(.*?)(?=\n\n[A-Z\s]{3,}:|\Z)"
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            # Clean up the extracted text
            opinion = match.group(1).strip()
            # Remove leading list-like characters (e.g., "1.", "a)", "-")
            opinion = re.sub(r'^\s*[\d\w][\.\)]\s*', '', opinion)
            return opinion

    return "Not Found"


# The RAG system is no longer needed, but we keep the RAG_AVAILABLE flag logic
# to gracefully handle its absence in the UI/backend calls.
RAG_AVAILABLE = False
try:
    # This block is kept for potential future re-activation but is currently disabled.
    # index = faiss.read_index(r"chola_faiss_index.faiss")
    # with open(r'chola_metadata.json', 'r', encoding='utf-8') as f:
    #     metadata = json.load(f)
    # with open(r'chola_answers.json', 'r', encoding='utf-8') as f:
    #     answers = json.load(f)
    # print(f"Loaded {len(metadata)} metadata entries and {len(answers)} answers")
    # RAG_AVAILABLE = True
    print("RAG system is disabled as per the new implementation.")
except Exception as e:
    print(f"RAG system files not found or disabled: {e}")
    RAG_AVAILABLE = False


def ensure_dir(directory):
    path = Path(directory)
    if not path.exists():
        try:
            path.mkdir(parents=True, exist_ok=True)
        except OSError as e:
            st.error(f"Failed to create directory {directory}: {e}")
            st.stop()

def check_poppler(poppler_path_config):
    if not poppler_path_config and platform.system() != "Windows" and not IS_MAC:
        if shutil.which("pdfinfo"): return True
        else:
            print("Poppler Check Failed: pdfinfo not found in PATH.")
            return False
    elif not poppler_path_config and platform.system() == "Windows":
        print("Poppler Check Failed: POPPLER_PATH not configured for Windows.")
        return False

    # For Mac, poppler_path_config is set, so this will run. For others, it checks the configured path.
    try:
        # On Mac, the config path is a directory, not a file path.
        if IS_MAC:
            pdfinfo_command = str(Path(poppler_path_config) / 'pdfinfo')
        else:
            pdfinfo_command = 'pdfinfo' if not poppler_path_config else str(Path(poppler_path_config) / 'pdfinfo')

        process = subprocess.Popen([pdfinfo_command, '-h'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=platform.system()=="Windows")
        _, err = process.communicate(timeout=5)
        if process.returncode in [0, 1]: return True
        else:
            print(f"Poppler Check Failed from '{poppler_path_config}'. RC: {process.returncode}. Err: {err.decode(errors='ignore')}")
            return False
    except FileNotFoundError:
        print(f"Poppler Check Failed: pdfinfo not found at '{poppler_path_config}'.")
        return False
    except subprocess.TimeoutExpired:
        print(f"Poppler Check Timed Out from '{poppler_path_config}'.")
        return False
    except Exception as e:
        print(f"Poppler Check Failed: Unexpected error: {e}. Path: '{poppler_path_config}'.")
        return False

def pdf_bytes_to_images(pdf_bytes, pdf_filename, poppler_path_config, dpi=450):
    images = []
    try:
        actual_poppler_path = poppler_path_config if poppler_path_config and Path(poppler_path_config).is_dir() else None
        images = convert_from_bytes(pdf_bytes, dpi=dpi, poppler_path=actual_poppler_path, thread_count=os.cpu_count() or 1, fmt='jpeg', timeout=600)
        if not images:
            print(f"Poppler converted '{pdf_filename}' but returned no images.")
            return []
    except Exception as e:
        print(f"Error converting PDF '{pdf_filename}' to images: {e}")
        if "poppler" in str(e).lower() or "pdfinfo" in str(e).lower() or "pdftoppm" in str(e).lower():
            print("Poppler issue. Check Poppler installation/PATH/POPPLER_PATH. PDF might be corrupted/protected.")
        return None
    return images

def ocr_image(image_pil: Image.Image, page_num: int, original_filename: str):
    """
    Performs OCR on a single image using the Doctr model.
    Tesseract logic has been removed as requested.
    """
    page_all_texts_untranslated = []
    debug_log_messages = []

    if not DOCTR_AVAILABLE or not DOCTR_PREDICTOR:
        msg = f"Doctr OCR Error (p{page_num}, {original_filename}): Doctr model is not available or not loaded."
        debug_log_messages.append(msg)
        print(f"ERROR: ocr_image (Doctr): {msg}")
        return f"[Doctr OCR Error on page {page_num}: Model not available]", debug_log_messages

    debug_log_messages.append(f"Page {page_num}: Using Doctr OCR engine.")
    print(f"DEBUG: ocr_image (Page {page_num}, File {original_filename}): Using Doctr engine.")
    try:
        # Convert PIL Image to numpy array as required by doctr
        doc_page_np = [np.array(image_pil.convert('RGB'))]
        result = DOCTR_PREDICTOR(doc_page_np)
        json_output = result.export()

        if json_output and json_output.get('pages'):
            page_data = json_output['pages'][0]

            # Reconstruct full text from all detected blocks for a natural reading order
            page_content_blocks = []
            for block in page_data.get('blocks', []):
                block_text = ""
                for line in block.get('lines', []):
                    # Join words in a line with a space
                    line_text = ' '.join([word['value'] for word in line.get('words', [])])
                    block_text += line_text + '\n' # Add newline after each line
                page_content_blocks.append(block_text.strip())

            full_page_text = "\n\n".join(page_content_blocks)
            page_all_texts_untranslated.append(f"\n--- Full Page OCR Text (Doctr) (p{page_num}) ---\n{full_page_text}\n--- End Full Page OCR Text ---\n")
            debug_log_messages.append(f"  Page {page_num}: Doctr full page OCR successful. Length: {len(full_page_text)}")

            # Extract tables detected by Doctr's layout analysis
            if page_data.get('tables'):
                debug_log_messages.append(f"  Page {page_num}: Doctr detected {len(page_data['tables'])} table(s).")
                for table_idx, table_structure in enumerate(page_data['tables']):
                    table_as_string = ""
                    # Reconstruct the table from its cell structure
                    for row in table_structure.get('body', []):
                        row_cells = [cell.get('value', '') for cell in row.get('cells', [])]
                        table_as_string += " | ".join(row_cells) + "\n"

                    demarcated_table_text = f"\n--- Start of AI Detected Table (Doctr p{page_num}-t{table_idx+1}) ---\n{table_as_string.strip()}\n--- End of AI Detected Table (Doctr) ---\n"
                    page_all_texts_untranslated.append(demarcated_table_text)
                    debug_log_messages.append(f"    Page {page_num}: Extracted Doctr table {table_idx+1}.")
            else:
                debug_log_messages.append(f"  Page {page_num}: Doctr detected no tables on this page.")
        else:
            debug_log_messages.append(f"  Page {page_num}: Doctr OCR returned no pages in its output.")
            page_all_texts_untranslated.append(f"[Doctr returned no content for page {page_num}]")

    except Exception as e:
        msg = f"Doctr OCR Error (p{page_num}, {original_filename}): {e}"
        debug_log_messages.append(msg)
        print(f"ERROR: ocr_image (Doctr): {msg}\n{traceback.format_exc()}")
        page_all_texts_untranslated.append(f"[Doctr OCR Error on page {page_num}]")

    # --- Final Text Combination ---
    final_combined_page_text = "\n\n".join(pt for pt in page_all_texts_untranslated if pt.strip() and not pt.startswith("[No text found")).strip()
    if not final_combined_page_text:
        final_combined_page_text = f"[No text extracted from page {page_num} via Doctr engine]"
        debug_log_messages.append(f"Page {page_num}: Final combined text is empty.")

    return final_combined_page_text, debug_log_messages

def extract_text_from_pdf(file_path_obj: Path):
    try:
        from PyPDF2 import PdfReader
        reader = PdfReader(str(file_path_obj))
        if reader.is_encrypted:
            try:
                if reader.decrypt("") == 0:
                    print(f"PDF '{file_path_obj.name}' encrypted, decryption failed.")
                    return "[PDF Encrypted - Decryption Failed]"
            except Exception as decrypt_e:
                print(f"Error decrypting PDF '{file_path_obj.name}': {decrypt_e}")
                return "[PDF Encrypted - Decryption Error]"
        text_content = []
        num_pages_with_text = 0
        for page_num, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text()
                if page_text and page_text.strip():
                    text_content.append(f"--- Page {page_num + 1} ---\n{page_text.strip()}")
                    num_pages_with_text += 1
            except Exception as page_err:
                print(f"PyPDF2: Error extracting text from page {page_num + 1} of '{file_path_obj.name}': {page_err}")
                text_content.append(f"--- Page {page_num + 1} ---\n[Text Extraction Error on this page]")
        if num_pages_with_text > 0 :
                        print(f"PyPDF2: Found parsable text on {num_pages_with_text}/{len(reader.pages)} pages of '{file_path_obj.name}'.")
                        return "\n\n".join(text_content)
        else:
            print(f"PyPDF2: No parsable text in '{file_path_obj.name}'. OCR needed.")
            return None
    except ImportError:
                print("PyPDF2 not installed. pip install PyPDF2")
                return "[PyPDF2 Import Error]"
    except Exception as e:
        print(f"PyPDF2: Error extracting text from PDF '{file_path_obj.name}': {e}. OCR needed.")
        return None

def extract_tables_with_tabula(pdf_path_obj: Path, filename_str: str):
    all_tables_dfs_with_pages = []
    plain_text_tables_for_llm = []
    embedded_html_tables_str = "<!-- No tables attempted or extracted -->"
    print(f"  -> Starting Tabula table extraction for '{filename_str}'...")
    java_home = os.environ.get("JAVA_HOME")
    java_exe_found = False
    if java_home:
        if platform.system() == "Windows" and (Path(java_home) / "bin" / "java.exe").exists(): java_exe_found = True
        elif platform.system() != "Windows" and (Path(java_home) / "bin" / "java").exists(): java_exe_found = True
    if not java_exe_found and not shutil.which("java"):
        print(f"Tabula: Java runtime not found. Table extraction will fail.")
        return "<!-- Error: Tabula - Java Not Found -->\n", [], []
    try:
        dfs_from_pdf = tabula.read_pdf(str(pdf_path_obj), pages='all', multiple_tables=True,
                                       stream=True, lattice=True, silent=True, encoding='utf-8',
                                       java_options=["-Dfile.encoding=UTF8"])
        if not dfs_from_pdf:
            print(f"  Tabula: No tables detected in '{filename_str}'.")
            return "<!-- No tables detected -->\n", [], []
        temp_dfs_with_pages = []
        for i, df in enumerate(dfs_from_pdf):
            if isinstance(df, pd.DataFrame) and not df.empty:
                df.dropna(axis=0, how='all', inplace=True)
                df.dropna(axis=1, how='all', inplace=True)
                if not df.empty: temp_dfs_with_pages.append((df.fillna(''), 'Unknown'))
        if not temp_dfs_with_pages:
            print(f"  Tabula: No tables detected after cleaning in '{filename_str}'.")
            return "<!-- No valid tables after cleaning -->\n", [], []
        print(f"  Tabula: Found {len(temp_dfs_with_pages)} potential table(s) in '{filename_str}'. Formatting...")
        all_tables_dfs_with_pages = temp_dfs_with_pages
        embedded_html_tables_str = "\n\n<!-- Tabula Extracted Tables Start -->\n"
        for i, (df, page_num_str) in enumerate(all_tables_dfs_with_pages):
            if df.empty: continue
            table_id_html = f"Table {i+1} (Page: {page_num_str})"
            table_id_plain = f"--- Table {i+1} (Page: {page_num_str}) ---"
            try: embedded_html_tables_str += f"<!-- HTML Table Start: {table_id_html} -->\n{df.to_html(index=False, border=1, classes='tabula-table streamlit-table', na_rep='')}\n<!-- HTML Table End: {table_id_html} -->\n\n"
            except Exception as html_err: print(f"Could not convert table {i+1} from '{filename_str}' to HTML: {html_err}")
            try:
                with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 2000, 'display.max_colwidth', None):
                    plain_text_table = df.to_string(index=False, na_rep='')
                plain_text_tables_for_llm.append(f"{table_id_plain}\n{plain_text_table}")
            except Exception as plain_err: print(f"Could not convert table {i+1} from '{filename_str}' to plain text: {plain_err}")
        embedded_html_tables_str += "<!-- Tabula Extracted Tables End -->\n\n"
        return embedded_html_tables_str, all_tables_dfs_with_pages, plain_text_tables_for_llm
    except FileNotFoundError as e:
        if "java" in str(e).lower(): print(f"Tabula: Java runtime not found. Error: {e}. Extraction failed.")
        else: print(f"Tabula: File not found error for '{filename_str}': {e}")
        return "<!-- Error: Tabula - Java/File Not Found -->\n", [], []
    except Exception as e:
        print(f"Tabula: Unexpected error for '{filename_str}': {e}.")
        print(traceback.format_exc())
        return f"<!-- Error: Tabula failed for {filename_str} -->\n", [], []

def clean_processing_artifacts(text, filename_for_debug=""):
    if not text: return ""
    cleaned_text = text
    patterns_to_remove_at_start = [
        re.compile(r"^\s*Okay, I'm ready to process.*?\.\s*", re.IGNORECASE | re.DOTALL),
        re.compile(r"^\s*Okay, I understand.*?Here's the Markdown table with the extracted data\.\s*", re.IGNORECASE | re.DOTALL),
        re.compile(r"^\s*Here's the Markdown table.*?\.\s*", re.IGNORECASE | re.DOTALL),
        re.compile(r"^\s*Here is the English translation.*?\n", re.IGNORECASE | re.DOTALL),
        re.compile(r"^\s*Here's the English translation.*?\n", re.IGNORECASE | re.DOTALL),
        re.compile(r"^\s*Here's a translation of the provided text.*?\n", re.IGNORECASE | re.DOTALL),
        re.compile(r"^\s*This document appears to be.*?translation.*?\n", re.IGNORECASE | re.DOTALL),
        re.compile(r"^\s*This appears to be nonsensical text.*?\n", re.IGNORECASE | re.DOTALL),
        re.compile(r"^\s*This appears to be a collection of.*original text to translate\..*?\n", re.IGNORECASE | re.DOTALL),
        re.compile(r"^\s*I apologize, but.*?fluent English translation due to the significant damage.*\n", re.IGNORECASE | re.MULTILINE),
        re.compile(r"^\s*```json\s*$", re.IGNORECASE | re.MULTILINE),
        re.compile(r"^\s*Here is the JSON output:?\s*```json\s*", re.IGNORECASE | re.DOTALL),
        re.compile(r"^\s*```\s*$", re.IGNORECASE | re.MULTILINE),
    ]
    for pattern in patterns_to_remove_at_start:
        cleaned_text = pattern.sub("", cleaned_text, count=1).lstrip()
    cleaned_text = re.sub(r"\[Information redacted.*?\]", "", cleaned_text, flags=re.IGNORECASE)
    cleaned_text = re.sub(r"\[.*?fragmented text.*?\]", "", cleaned_text, flags=re.IGNORECASE | re.DOTALL)
    cleaned_text = re.sub(r"\[\s*Content of image.*?could not be reliably translated\s*\]", "", cleaned_text, flags=re.IGNORECASE)
    cleaned_text = re.sub(r"\[.*?No translation available due to .*?\]", "", cleaned_text, flags=re.IGNORECASE | re.DOTALL)
    cleaned_text = re.sub(r'\r\n', '\n', cleaned_text)
    cleaned_text = re.sub(r'\r', '\n', cleaned_text)
    cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text).strip()
    return cleaned_text

def count_non_english_chars(text):
    if not text: return 0
    return sum(1 for char in text if ord(char) > 127)

def extract_address_components(text):
    """Extracts individual address components from the document text using regex."""
    address_components = {}
    for component, pattern in COMPONENT_PATTERNS.items():
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            # For patterns with multiple capture groups (like street, village, etc)
            captured_value = next((group for group in match.groups() if group), None)
            if captured_value:
                address_components[component] = captured_value.strip()

    return address_components

def construct_address(address_components):
    """Combines extracted address components into a standardized address string."""
    address_parts = []
    door_survey = []
    if "door_no" in address_components:
        door_survey.append(f"Door No. {address_components['door_no']}")
    if "survey_no" in address_components:
        door_survey.append(f"Survey No. {address_components['survey_no']}")
    if door_survey:
        address_parts.append("/".join(door_survey))
    if "street" in address_components:
        address_parts.append(address_components['street'])
    if "district" in address_components:
        address_parts.append(address_components['district'])
    if "state" in address_components:
        address_parts.append(address_components['state'])
    return ", ".join(address_parts) if address_parts else "Not Found"


# --- Document Processing Functions ---
def process_pdf_file(file_bytes, filename, poppler_path_config, ocr_langs):
    # Set OCR engine to Doctr permanently
    ocr_engine = "Doctr"
    print(f"--- Starting PDF Pre-processing for: {filename} (OCR Engine: {ocr_engine}) ---")
    final_english_text = ""
    original_untranslated_text = ""
    processing_notes = []
    detected_language_info = {"code": "unknown", "name": "Unknown", "source": "unknown"}
    embedded_html_tables = "<!-- No tables attempted or extracted yet -->"
    all_tables_dfs_with_pages = []
    plain_text_tables_for_llm = []
    processing_error = None
    processing_warning = None
    ensure_dir(OUTPUT_DIR)
    temp_pdf_path = Path(OUTPUT_DIR) / f"temp_{Path(filename).stem}_{datetime.now().strftime('%Y%m%d%H%M%S%f')}.pdf"
    try:
        with open(temp_pdf_path, "wb") as temp_file: temp_file.write(file_bytes)
        print(f"  Saved temporary PDF to '{temp_pdf_path.name}'")
    except Exception as e:
        print(f"Failed to save temporary PDF for {filename}: {e}")
        return {"error": f"Failed to save temp PDF: {e}", "translation": "[Error Saving Temp PDF]", "original_text": "", "notes": [f"Failed to save temp PDF: {e}"], "detected_language": {"code":"error", "name":"Error", "source":"system"}, "tables_dfs": [], "embedded_html_tables": "<!-- File Save Error -->", "plain_text_tables": []}
    try:
        extracted_text_pypdf = extract_text_from_pdf(temp_pdf_path)
        if extracted_text_pypdf and not extracted_text_pypdf.startswith("["):
            original_untranslated_text = extracted_text_pypdf
            processing_notes.append("Used parsable text from PDF (PyPDF2).")
            detected_language_info["source"] = "direct_pdf_text"
            if original_untranslated_text:
                safe_stem_direct = "".join(c if c.isalnum() or c in ('_', '-') else '_' for c in Path(filename).stem)
                direct_untranslated_save_filename = Path(OUTPUT_DIR) / f"untranslated_direct_pdf_{safe_stem_direct}.txt"
                try:
                    with open(direct_untranslated_save_filename, "w", encoding="utf-8") as f_direct:
                        f_direct.write(f"# Untranslated Direct Text (PyPDF2) from: {filename} @ {datetime.now().isoformat()}\n# ---\n\n{original_untranslated_text}")
                    note_msg = f"Saved untranslated direct PDF text to '{direct_untranslated_save_filename.name}'."
                    processing_notes.append(note_msg); print(f"  {note_msg}")
                except Exception as direct_save_e:
                    err_msg = f"Untranslated direct PDF text save error for '{filename}': {direct_save_e}"
                    processing_notes.append(err_msg); print(f"  ERROR: {err_msg}")
            final_english_text = original_untranslated_text
            non_english_char_count = count_non_english_chars(original_untranslated_text)
            if non_english_char_count > NON_ENGLISH_CHAR_THRESHOLD:
                try:
                    lang_info = translator.identify_language(original_untranslated_text)
                    if not isinstance(lang_info, dict) or 'code' not in lang_info: lang_info = {"code": "en", "name": "English (Defaulted on ID error)"}
                    detected_language_info.update(lang_info)
                    if lang_info.get("code", "en") != "en":
                        print(f"  Translating direct PDF text from {lang_info.get('name','Unknown Language')}...")
                        translated_text = translator.translate_to_english(original_untranslated_text, lang_info)
                        final_english_text = translated_text if translated_text and translated_text.strip() else original_untranslated_text
                        processing_notes.append(f"Translated direct PDF text from {lang_info.get('name','Unknown Language')}.")
                    else: processing_notes.append("Direct PDF text identified as English.")
                except Exception as lang_e:
                    print(f"  Language ID/Translation error for direct PDF text: {lang_e}. Using untranslated text.")
                    processing_notes.append(f"Language ID/Translation error (direct PDF text): {lang_e}")
                    detected_language_info.update({"code":"error", "name": f"Error ({lang_e})"})
            else:
                detected_language_info.update({"code": "en", "name": "English (assumed from direct PDF text)"})
                processing_notes.append(f"Assumed English for direct PDF text (low non-ASCII: {non_english_char_count}).")
        elif extracted_text_pypdf and (extracted_text_pypdf.startswith("[PDF Encrypted") or extracted_text_pypdf.startswith("[PyPDF2 Import Error]")):
            processing_error = extracted_text_pypdf[1:-1]
            original_untranslated_text = final_english_text = extracted_text_pypdf
            processing_notes.append(processing_error)
            detected_language_info["source"] = "direct_pdf_error"
        else: # Path 2: PyPDF2 failed or no text, proceed to OCR
            processing_notes.append(f"No parsable text from PyPDF2 or error, proceeding to OCR with '{ocr_engine}'.")
            original_untranslated_text = f"[OCR Attempted with {ocr_engine} as PyPDF2 Failed]"
            images = pdf_bytes_to_images(file_bytes, filename, poppler_path_config)
            if images is None:
                processing_error = "PDF to Image conversion failed."
                final_english_text = original_untranslated_text = "[PDF Conversion Error]"
                detected_language_info["source"] = "ocr_conversion_error"
            elif not images:
                processing_warning = "PDF converted to 0 images (empty PDF?)."
                final_english_text = original_untranslated_text = "[Empty PDF - No Pages for OCR]"
                detected_language_info["source"] = "ocr_empty_pdf"
            else: # OCR proceeds
                if not DOCTR_AVAILABLE or not DOCTR_PREDICTOR:
                     processing_error = "Doctr OCR Engine is not available or failed to load. Cannot process scanned PDF."
                     final_english_text = original_untranslated_text = "[OCR Engine Not Available]"
                     detected_language_info["source"] = "ocr_engine_error"
                else:
                    processing_notes.append(f"Attempting OCR on {len(images)} page(s) using '{ocr_engine}' engine.")
                    full_original_ocr_text_list = []
                    full_final_english_ocr_text_list = []
                    page_level_lang_info = {}
                    detected_language_info["source"] = "ocr"

                    for i, img in enumerate(images):
                        page_num = i + 1
                        page_ocr_untranslated_text, page_debug_logs = ocr_image(
                            img, page_num, filename
                        )
                        if page_debug_logs:
                            processing_notes.extend(page_debug_logs)
                        full_original_ocr_text_list.append(f"--- Page {page_num} ---\n{page_ocr_untranslated_text}")
                        page_final_english_segment = page_ocr_untranslated_text
                        if page_ocr_untranslated_text and not page_ocr_untranslated_text.startswith("["):
                            non_english_char_count_ocr = count_non_english_chars(page_ocr_untranslated_text)
                            current_page_lang_info = {"code": "en", "name": "English (assumed from OCR)"}
                            if non_english_char_count_ocr > NON_ENGLISH_CHAR_THRESHOLD:
                                try:
                                    lang_info_ocr_page = translator.identify_language(page_ocr_untranslated_text)
                                    if not isinstance(lang_info_ocr_page, dict) or 'code' not in lang_info_ocr_page: lang_info_ocr_page = {"code": "en", "name": "English (Defaulted on page ID error)"}
                                    current_page_lang_info = lang_info_ocr_page
                                    if lang_info_ocr_page.get("code", "en") != "en":
                                        print(f"  Translating OCR text from page {page_num} ({lang_info_ocr_page.get('name','Unknown')})...")
                                        page_translated = translator.translate_to_english(page_ocr_untranslated_text, lang_info_ocr_page)
                                        page_final_english_segment = page_translated if page_translated and page_translated.strip() else page_ocr_untranslated_text
                                        processing_notes.append(f"Translated OCR text from page {page_num} ({lang_info_ocr_page.get('name','Unknown')}).")
                                    else: processing_notes.append(f"OCR text on page {page_num} identified as English.")
                                except Exception as page_lang_e:
                                    print(f"  Lang ID/Trans error for OCR page {page_num}: {page_lang_e}. Using untranslated OCR.")
                                    processing_notes.append(f"Lang ID/Translation error (OCR page {page_num}): {page_lang_e}")
                            else: processing_notes.append(f"Assumed English OCR text on page {page_num} (low non-ASCII: {non_english_char_count_ocr}).")
                            page_level_lang_info[page_num] = current_page_lang_info
                        full_final_english_ocr_text_list.append(f"--- Page {page_num} ---\n{page_final_english_segment}")
                        try: img.close()
                        except: pass
                    original_untranslated_text = "\n\n".join(full_original_ocr_text_list)
                    final_english_text = "\n\n".join(full_final_english_ocr_text_list)
                    if page_level_lang_info:
                        first_page_with_lang = next((lang for p, lang in sorted(page_level_lang_info.items()) if lang.get("code") != "en" or lang.get("name") != "English (assumed from OCR)"), None)
                        if first_page_with_lang: detected_language_info.update(first_page_with_lang)
                        elif page_level_lang_info.get(1): detected_language_info.update(page_level_lang_info)
                    if original_untranslated_text and not original_untranslated_text.strip().startswith("["):
                        safe_stem_ocr = "".join(c if c.isalnum() or c in ('_', '-') else '_' for c in Path(filename).stem)
                        ocr_untranslated_save_filename = Path(OUTPUT_DIR) / f"untranslated_full_doc_ocr_{safe_stem_ocr}.txt"
                        try:
                            with open(ocr_untranslated_save_filename, "w", encoding="utf-8") as f_ocr:
                                f_ocr.write(f"# Untranslated Full Doc OCR from: {filename} @ {datetime.now().isoformat()}\n# OCR Langs: {ocr_langs}\n# OCR Engine: {ocr_engine}\n# ---\n\n{original_untranslated_text}")
                            note_msg = f"Saved untranslated full-doc raw OCR to '{ocr_untranslated_save_filename.name}'."
                            processing_notes.append(note_msg); print(f"  {note_msg}")
                        except Exception as ocr_save_e:
                            err_msg = f"Untranslated full-doc raw OCR save error for '{filename}': {ocr_save_e}"
                            processing_notes.append(err_msg); print(f"  ERROR: {err_msg}")
        # Note: Tabula runs independently of the OCR engine choice for now.
        html_tables_str, dfs_list, plain_tables_list = extract_tables_with_tabula(temp_pdf_path, filename)
        embedded_html_tables = html_tables_str
        all_tables_dfs_with_pages = dfs_list
        plain_text_tables_for_llm = plain_tables_list
        if plain_text_tables_for_llm: processing_notes.append(f"Tabula formatted {len(plain_text_tables_for_llm)} table(s).")
        else: processing_notes.append("No tables by Tabula or tables were empty.")
        if not processing_error and final_english_text and not final_english_text.startswith("["):
            cleaned_text = clean_processing_artifacts(final_english_text, filename_for_debug=filename)
            if cleaned_text != final_english_text: processing_notes.append("Applied artifact cleaning to final English text.")
            final_english_text = cleaned_text
        if not processing_error and final_english_text and not final_english_text.startswith("["):
            safe_stem = "".join(c if c.isalnum() or c in ('_', '-') else '_' for c in Path(filename).stem)
            processed_english_save_filename = Path(OUTPUT_DIR) / f"processed_english_for_llm_{safe_stem}.txt"
            try:
                with open(processed_english_save_filename, "w", encoding="utf-8") as f:
                    f.write(f"# Final Processed English Text (for LLM) from: {filename} @ {datetime.now().isoformat()}\n")
                    f.write(f"# Orig Text Source: {detected_language_info.get('source', 'unknown')}\n")
                    f.write(f"# Detected Lang: {detected_language_info.get('name','N/A')} ({detected_language_info.get('code','N/A')})\n")
                    f.write(f"# Tables (Plain Text): {len(plain_text_tables_for_llm)}\n# ---\n\n{final_english_text}\n\n")
                    if plain_text_tables_for_llm:
                        f.write("## Extracted Tables (Plain Text for LLM):\n")
                        for pt_table in plain_text_tables_for_llm: f.write(f"```text\n{pt_table}\n```\n\n")
                processing_notes.append(f"Saved final processed English text to {processed_english_save_filename.name}")
            except Exception as save_e: processing_notes.append(f"Save error (final English text): {save_e}")

        final_status = {"original_text": original_untranslated_text, "detected_language": detected_language_info, "translation": final_english_text, "tables_dfs": all_tables_dfs_with_pages, "embedded_html_tables": embedded_html_tables, "plain_text_tables": plain_text_tables_for_llm, "notes": processing_notes}
        if processing_error: final_status["error"] = processing_error
        if processing_warning: final_status["warning"] = processing_warning
        print(f"--- Finished PDF Pre-processing for: {filename} ---")
        return final_status
    except Exception as e:
        print(f"Critical PDF processing error '{filename}': {e}")
        detailed_tb = traceback.format_exc()
        print(f"Traceback: {detailed_tb}")
        return {"error": f"Critical PDF error: {e}", "translation": "[Critical PDF Error]", "original_text": f"[Critical PDF Error processing {filename}]", "notes": [f"Critical error: {e}", detailed_tb], "detected_language": {"code":"error", "name":"Error", "source":"system"}, "tables_dfs": [], "embedded_html_tables": "<!-- Critical Error -->", "plain_text_tables": []}
    finally:
        try:
            if temp_pdf_path.exists(): temp_pdf_path.unlink()
        except Exception as cleanup_error: print(f"Could not delete temp file '{temp_pdf_path.name}': {cleanup_error}")

def process_txt_file(file_bytes, filename):
    print(f"--- Starting TXT Pre-processing for: {filename} ---")
    original_text, final_english_text = "", ""
    notes, detected_language_info = [], {"code": "unknown", "name": "Unknown", "source": "unknown"}
    processing_error, processing_warning = None, None
    try:
        try: original_text = file_bytes.decode('utf-8'); notes.append("Decoded utf-8.")
        except UnicodeDecodeError:
            try: original_text = file_bytes.decode('latin-1'); notes.append("Decoded latin-1.")
            except UnicodeDecodeError as de:
                processing_error = f"Decode error: {de}"; print(f"Decode error for {filename}: {de}")
                return {"error": processing_error, "translation": "[Decode Error]", "original_text": "[Decode Error]", "notes": [processing_error], "detected_language": {"code":"error"}, "tables_dfs": [], "embedded_html_tables": "<!-- Decode Error -->", "plain_text_tables": []}
    except Exception as e:
        processing_error = f"Unexpected decode error: {e}"; print(f"Unexpected decode error for {filename}: {e}")
        return {"error": processing_error, "translation": "[Decode Error]", "original_text": "[Decode Error]", "notes": [processing_error], "detected_language": {"code":"error"}, "tables_dfs": [], "embedded_html_tables": "<!-- Decode Error -->", "plain_text_tables": []}
    if not original_text.strip():
        processing_warning = "TXT file empty/whitespace."; notes.append(processing_warning)
        final_english_text = "[File empty]"; print(f"TXT file '{filename}' is empty.")
    else:
        detected_language_info["source"] = "text_file"
        non_english_char_count = count_non_english_chars(original_text)
        if non_english_char_count > NON_ENGLISH_CHAR_THRESHOLD:
            try:
                lang_info = translator.identify_language(original_text)
                if not isinstance(lang_info, dict) or 'code' not in lang_info: lang_info = {"code": "en", "name": "English (Defaulted)"}
                detected_language_info.update(lang_info)
                if lang_info.get("code", "en") != "en":
                    print(f"  Translating TXT from {lang_info.get('name','Unknown')}...")
                    translated_text = translator.translate_to_english(original_text, lang_info)
                    final_english_text = translated_text if translated_text and translated_text.strip() else original_text
                    notes.append(f"Translated from {lang_info.get('name','Unknown')}.")
                else: final_english_text = original_text; notes.append("TXT identified as English.")
            except Exception as lang_e:
                print(f"  Lang ID/Trans error for TXT: {lang_e}. Using raw text.")
                final_english_text = original_text; notes.append(f"Lang ID/Trans error: {lang_e}")
        else:
                        final_english_text = original_text
                        detected_language_info.update({"code": "en", "name": "English (assumed)"})
                        notes.append(f"Assumed English (low non-ASCII: {non_english_char_count}).")
        if final_english_text and not final_english_text.startswith("["):
            cleaned_text = clean_processing_artifacts(final_english_text, filename_for_debug=filename)
        if cleaned_text != final_english_text: notes.append("Applied artifact cleaning.")
        final_english_text = cleaned_text
    if final_english_text and not final_english_text.startswith("["):
        ensure_dir(OUTPUT_DIR)
        safe_stem = "".join(c if c.isalnum() or c in ('_', '-') else '_' for c in Path(filename).stem)
        save_filename = Path(OUTPUT_DIR) / f"processed_english_for_llm_{safe_stem}.txt"
        try:
            with open(save_filename, "w", encoding="utf-8") as f:
                f.write(f"# Processed from: {filename} @ {datetime.now().isoformat()}\n# Lang ({detected_language_info.get('source', '?')}): {detected_language_info.get('name','N/A')} ({detected_language_info.get('code','N/A')})\n# ---\n\n{final_english_text}")
            notes.append(f"Saved to {save_filename.name}")
        except Exception as save_e: notes.append(f"Save error: {save_e}")

    final_result = {"original_text": original_text, "translation": final_english_text, "detected_language": detected_language_info, "notes": notes, "tables_dfs": [], "embedded_html_tables": "<!-- No tables for TXT -->", "plain_text_tables": []}
    if processing_error: final_result["error"] = processing_error
    if processing_warning: final_result["warning"] = processing_warning
    print(f"--- Finished TXT Pre-processing for: {filename} ---")
    return final_result

# Vertex AI integration code
from google import genai
from google.genai import types
import os
import json
import base64
import fitz  # PyMuPDF for PDF handling
import tempfile
from PIL import Image, ImageDraw, ImageFont
import io
import docx
import config

def process_document(file_path, prompt, project_id=config.VERTEX_AI_PROJECT_ID,
                    location=config.VERTEX_AI_LOCATION,
                    credentials_path=config.CREDENTIALS_PATH):
    """
    Process a document (image, PDF, DOCX, or TXT) using Gemini to extract structured information.
    
    Args:
        file_path: Path to the document file
        prompt: The prompt to send to Gemini
        project_id: Google Cloud project ID
        location: Google Cloud region
        credentials_path: Path to the service account credentials file
        
    Returns:
        Structured JSON data with extracted information
    """
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = credentials_path
    client = genai.Client(vertexai=True, project=project_id, location=location)
    model = config.VERTEX_AI_MODEL
    
    # Determine file type
    file_extension = os.path.splitext(file_path).lower()
    
    # Process based on file type
    if file_extension in ['.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp']:
        # For image files, process directly
        return process_image_file(file_path, client, model, prompt)
    elif file_extension == '.pdf':
        # For PDF files, process the first page
        return process_pdf_file(file_path, client, model, prompt)
    elif file_extension in ['.doc', '.docx']:
        # For Word documents, convert to images and process
        return process_docx_file(file_path, client, model, prompt)
    elif file_extension == '.txt':
        # For TXT files, convert to image and process
        return process_txt_file(file_path, client, model, prompt)
    else:
        raise ValueError(f"Unsupported file format: {file_extension}")

def process_image_file(image_path, client, model, prompt):
    """Process an image file using Gemini."""
    # Read and encode the image
    with open(image_path, "rb") as image_file:
        image_bytes = image_file.read()
        image_base64 = base64.b64encode(image_bytes).decode('utf-8')
    
    # Process the image
    return generate_content_with_image(client, model, prompt, image_base64, "image/jpeg")
def process_pdf_file(pdf_path, client, model, prompt):
    """Process a PDF file using Gemini."""
    try:
        # Open the PDF
        pdf_document = fitz.open(pdf_path)
        
        # Check if PDF has pages
        if len(pdf_document) == 0:
            raise ValueError("The PDF document contains no pages")
        
        # Process all pages instead of just the first one
        all_pages_images = []
        max_pages = min(len(pdf_document), 15)  # Limit to 15 pages to avoid token limits
        
        for page_num in range(max_pages):
            page = pdf_document.load_page(page_num)
            
            # Create a unique temp file name for each page
            temp_img_path = os.path.join(tempfile.gettempdir(), f"pdf_page_{page_num}_{os.path.basename(pdf_path)}.png")
            
            # Render page to pixmap
            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # Higher resolution for better OCR
            
            # Save pixmap to temporary file
            pix.save(temp_img_path)
            
            # Read the temporary file and encode as base64
            with open(temp_img_path, "rb") as img_file:
                image_bytes = img_file.read()
                image_base64 = base64.b64encode(image_bytes).decode('utf-8')
                all_pages_images.append({
                    "mime_type": "image/png",
                    "data": image_base64
                })
            
            # Clean up temporary file
            try:
                os.remove(temp_img_path)
            except:
                pass
        
        # Close the PDF
        pdf_document.close()
        
        # If we have multiple pages, process them all together
        if len(all_pages_images) > 1:
            return generate_content_with_multiple_images(client, model, prompt, all_pages_images)
        elif len(all_pages_images) == 1:
            # If only one page, use the existing function
            return generate_content_with_image(client, model, prompt, all_pages_images["data"], all_pages_images["mime_type"])
        else:
            raise ValueError("No pages were successfully processed from the PDF")
    
    except Exception as e:
        raise RuntimeError(f"Error processing PDF: {e}")
def generate_content_with_multiple_images(client, model, prompt, images_data):
    """Generate content using Gemini with multiple images."""
    try:
        # Configure the model
        config = types.GenerateContentConfig(
            temperature=0.0,
            top_p=0.95,
            max_output_tokens=8192,
            response_mime_type="application/json",
            safety_settings=[types.SafetySetting(category=c, threshold="OFF") for c in [
                "HARM_CATEGORY_HATE_SPEECH", "HARM_CATEGORY_DANGEROUS_CONTENT",
                "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_HARASSMENT"]]
        )
        
        # Enhance the prompt to handle multiple pages
        enhanced_prompt = f"""
        {prompt}
        
        IMPORTANT: I'm providing multiple pages from the document. Please analyze ALL pages together to extract the complete information.
        Make sure to look across all pages for relevant details before finalizing your response.
        """
        
        # Create the content parts with all images
        content_parts = [{"text": enhanced_prompt}]
        
        # Add all images to the content parts
        for img_data in images_data:
            content_parts.append({
                "inline_data": {
                    "mime_type": img_data["mime_type"],
                    "data": img_data["data"]
                }
            })
        
        # Generate content
        response = client.models.generate_content(
            model=model,
            contents=content_parts,
            config=config
        )
        
        print(f"Processing complete for {len(images_data)} pages!")
        
        # Extract and parse the JSON response
        try:
            # Clean the response text to ensure it's valid JSON
            response_text = response.text.strip()
            
            # If response is wrapped in code blocks, extract just the JSON
            if response_text.startswith("```json"):
                response_text = response_text.split("```json").split("```")[0].strip()
            elif response_text.startswith("```"):
                response_text = response_text.split("```")[1].split("```").strip()
                
            # Parse the JSON
            result = json.loads(response_text)
            
            # Return both the structured data and the raw response
            return {
                "structured_data": result,
                "raw_response": response.text
            }
            
        except json.JSONDecodeError as e:
            print(f"Error parsing JSON response: {e}")
            print("Raw response:", response.text)
            return {
                "structured_data": None,
                "raw_response": response.text,
                "error": f"Failed to parse JSON: {e}"
            }
        
    except Exception as e:
        print(f"Error generating response: {e}")
        raise RuntimeError(f"Error generating response: {e}")
def process_docx_file(docx_path, client, model, prompt):
    """Process a DOCX file using Gemini."""
    try:
        # Create a temporary image file
        temp_img_path = os.path.join(tempfile.gettempdir(), f"docx_{os.path.basename(docx_path)}.png")
        
        # Extract text from DOCX
        doc = docx.Document(docx_path)
        text_content = "\n".join([para.text for para in doc.paragraphs])
        
        # Create an image with the text (simple approach)
        
        # Create a blank image
        img = Image.new('RGB', (1000, 1500), color=(255, 255, 255))
        d = ImageDraw.Draw(img)
        
        # Use default font
        try:
            font = ImageFont.truetype("arial.ttf", 16)
        except:
            font = ImageFont.load_default()
        
        # Draw text on image
        d.text((20, 20), text_content, fill=(0, 0, 0), font=font)
        
        # Save the image
        img.save(temp_img_path)
        
        # Read and encode the image
        with open(temp_img_path, "rb") as img_file:
            image_bytes = img_file.read()
            image_base64 = base64.b64encode(image_bytes).decode('utf-8')
        
        # Clean up temporary file
        try:
            os.remove(temp_img_path)
        except:
            pass
        
        # Process the image
        return generate_content_with_image(client, model, prompt, image_base64, "image/png")
    
    except Exception as e:
        raise RuntimeError(f"Error processing DOCX: {e}")

def process_txt_file(txt_path, client, model, prompt):
    """Process a TXT file using Gemini by converting to image."""
    try:
        # Create a temporary image file
        temp_img_path = os.path.join(tempfile.gettempdir(), f"txt_{os.path.basename(txt_path)}.png")
        
        # Read text from TXT
        with open(txt_path, 'r', encoding='utf-8') as f:
            text_content = f.read()
        
        # Create an image with the text
        img = Image.new('RGB', (1000, 1500), color=(255, 255, 255))
        d = ImageDraw.Draw(img)
        
        # Use default font
        try:
            font = ImageFont.truetype("arial.ttf", 16)
        except:
            font = ImageFont.load_default()
        
        # Draw text on image
        d.text((20, 20), text_content, fill=(0, 0, 0), font=font)
        
        # Save the image
        img.save(temp_img_path)
        
        # Read and encode the image
        with open(temp_img_path, "rb") as img_file:
            image_bytes = img_file.read()
            image_base64 = base64.b64encode(image_bytes).decode('utf-8')
        
        # Clean up temporary file
        try:
            os.remove(temp_img_path)
        except:
            pass
        
        # Process the image
        return generate_content_with_image(client, model, prompt, image_base64, "image/png")
    
    except Exception as e:
        raise RuntimeError(f"Error processing TXT: {e}")

def generate_content_with_image(client, model, prompt, image_base64, mime_type):
    """Generate content using Gemini with an image."""
    try:
        # Configure the model
        config = types.GenerateContentConfig(
            temperature=0.0,  # Lower temperature for more deterministic output
            top_p=0.95,
            max_output_tokens=8192,
            response_mime_type="application/json",
            safety_settings=[types.SafetySetting(category=c, threshold="OFF") for c in [
                "HARM_CATEGORY_HATE_SPEECH", "HARM_CATEGORY_DANGEROUS_CONTENT",
                "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_HARASSMENT"]]
        )
        
        # Create the content parts
        content_parts = [
            {"text": prompt},
            {"inline_data": {"mime_type": mime_type, "data": image_base64}}
        ]
        
        # Generate content
        response = client.models.generate_content(
            model=model,
            contents=content_parts,
            config=config
        )
        
        print("Processing complete!")
        
        # Extract and parse the JSON response
        try:
            # Clean the response text to ensure it's valid JSON
            response_text = response.text.strip()
            
            # If response is wrapped in code blocks, extract just the JSON
            if response_text.startswith("```json"):
                response_text = response_text.split("```json").split("```")[0].strip()
            elif response_text.startswith("```"):
                response_text = response_text.split("```")[1].split("```").strip()
                
            # Parse the JSON
            result = json.loads(response_text)
            
            # Return both the structured data and the raw response
            return {
                "structured_data": result,
                "raw_response": response.text
            }
            
        except json.JSONDecodeError as e:
            print(f"Error parsing JSON response: {e}")
            print("Raw response:", response.text)
            return {
                "structured_data": None,
                "raw_response": response.text,
                "error": f"Failed to parse JSON: {e}"
            }
        
    except Exception as e:
        print(f"Error generating response: {e}")
        raise RuntimeError(f"Error generating response: {e}")

def generate_text_content(client, model, prompt):
    """Generate text content using Gemini without an image."""
    try:
        # Configure the model
        config = types.GenerateContentConfig(
            temperature=0.0,
            top_p=0.95,
            max_output_tokens=8192,
            response_mime_type="text/plain",  # For text response
            safety_settings=[types.SafetySetting(category=c, threshold="OFF") for c in [
                "HARM_CATEGORY_HATE_SPEECH", "HARM_CATEGORY_DANGEROUS_CONTENT",
                "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_HARASSMENT"]]
        )
        
        # Create the content parts (text only)
        content_parts = [{"text": prompt}]
        
        # Generate content
        response = client.models.generate_content(
            model=model,
            contents=content_parts,
            config=config
        )
        
        print("Text generation complete!")
        
        # Return the cleaned response text
        return clean_processing_artifacts(response.text)
        
    except Exception as e:
        print(f"Error generating text response: {e}")
        raise RuntimeError(f"Error generating text response: {e}")

def create_single_doc_extraction_prompt(filename):
    """
    Creates the first prompt for the LLM, focusing ONLY on data extraction.
    The question generation part has been removed.
    """
    prompt = f"""You are an expert Legal Document Analyst AI. Your task is to perform a high-precision extraction from the provided document file '{filename}'. You must generate a single JSON object as output.

**PRIMARY OBJECTIVE**: Focus all your analytical power on flawlessly extracting the following sections. Accuracy and completeness for these sections are paramount.
1.  **PROPERTY ADDRESS**: The full, complete address of the primary property.
2.  **PROPERTY EXTENT DETAILS**: All measurements of the property.
3.  **BOUNDARIES**: The four cardinal boundaries (North, South, East, West).
4.  **TITLE HISTORY**: The chronological flow of ownership.
5.  **DOCUMENTS EXAMINED**: The list of all legal documents reviewed.
6.  **MORTGAGE DOCUMENTS**: Specific documents for pre and post-disbursal.
7.  **LEGAL OPINION**: A detailed, initial legal assessment based *only* on the text.

**DETAILED EXTRACTION INSTRUCTIONS**:

Extract the information into a single JSON object with the following keys. Adhere strictly to the formats provided.

1.  **PROPERTY ADDRESS** (High Priority):
   - A Property address is a identifier to a property which usually contains Plot numbers, Survey Numbers and more. Look for property address with most Identification numbers. Do not take address which is in R/O that is usually associated with Owner name or their details. Look for actual Property details with expanded details.
   - First, check 'Extracted Tables (Plain Text)' for address information.
   - Then, check 'General Text Content (English)'. Look for "DESCRIPTION OF THE PROPERTY", "Property Information", "Address", "SCHEDULE","SCHEDULE: OF PROPERTY", "DETAILS OF PROPERTY".
   - Extract the most complete physical address with property identification numbers, a complete address will usually be in the format consider this as an Example only and the order might change PLOT Number, Some vernacular property identification Number, a street or nagar name ,a village or town name,a district name, a state name and pin code in 6 digits.
   - House number/Door number/Unit number look for terms: "House No.", "H.No.", "Door No.", "Unit No.", "Building No." .
   - Survey number/Plot number/Patta number look for terms: "Survey No.", "S.No.", "Plot No.", "Patta No.", "Khata No." .
   - Indian vernacular terms like "khasra number", "Khewat", "Khatauni", "Patta", "Jamabandi", "Fard" numbers look for the address context and extract any of the vernacular property identification numbers
   - Street, locality, city, and PIN code
   - Combine all these elements into a single address string.
   - If multiple addresses exist, choose the one that appears in the main property description section
   - Format: A single string with all combined elements separated by commas.

2.  **BOUNDARIES** (High Priority):
    *   Extract the literal description for North, South, East, and West boundaries.
    *   Capture exactly what is written, e.g., "North: Property of [name]", "South: [measurement] Public Road".
    *   If a boundary is missing, use "Not Found" for that specific direction.
    *   If more than one set of boundaries are present in the document extract them all as separate objects in the list.
    *   Format: [{{"north": "[description]", "south": "[description]", "east": "[description]", "west": "[description]"}}, ...]
3.  **OWNER NAME(S)**:
    *   Identify the name(s) of the current legal owner(s) of the property.
    *   List multiple owners separated by commas.
    *   Format: "[Name1], [Name2]"

4.  **PROPERTY EXTENT DETAILS** (High Priority):
    *   Extract all available measurements of the property.
    *   Capture the original text exactly, including units (e.g., "[number] acres", "[number] sq. ft.", "[number] cents", "[number] grounds").
    *   Format: {{"land_area": "e.g., [number] acres", "built_up_area": "e.g., [number] sq. ft.", "floor_wise_areas": "e.g., Ground Floor: [number] sq. ft., First Floor: [number] sq. ft."}}
    *   If a measurement is not mentioned, use "Not Found".
5.  **DOCUMENTS EXAMINED** (High Priority):
    *   Extract the entire information from the "Documents Examined", "Documents Scrutinized", "Documents Verified", "Documents Reviewed", or any similar section exactly as it appears in the document. This is critical: capture EVERY SINGLE DOCUMENT listed, including all details provided for each one, without summarizing, filtering, omitting duplicates, or altering any content‚Äîeven if they are photocopies, originals, or repeated entries.
    *   Preserve the full description, including any dates, parties involved, document numbers, and other details verbatim.
    *   If a document has a number (e.g., Doc No [number]/[year]), include it in the document_name field as "Sale Deed Doc No [number]/[year]".
    *   If no specific dates or parties are mentioned for a document, use "Not Specified" for those fields.
    *   Format: [{{"document_name": "Sale Deed Doc No [number]/[year]", "creation_date": "DD-MM-YYYY", "execution_date": "DD-MM-YYYY", "executed_by": "[Issued by]", "in_favour_of": "[Receiver ]", "full_description": "[Verbatim full text/description of the document entry from the document]"}}]

6.  **TITLE HISTORY (CHRONOLOGICAL ORDER)** (High Priority):
    *   Trace the flow of ownership from the earliest document to the latest.
    *   Each entry in the list represents a transfer event (e.g., a sale, gift, partition).
    *   Critical : Extract all the transactions in title flow even if it points to multiple properites in the document but differentiate it.
    *   Every transcation revolving arround the property should be conisdered even if a small extent is sold it counts so extract all the transcation in title flow. Even draft settlement deed counts as a transcation.
    *   Ensure no transactions are skipped; include every mentioned transfer in the chain, with no data left out.
    *   Maintain the exact chronological order found in the document.
    *   Include the document number if mentioned (e.g., Doc No [number]/[year]) in the document_name field as "Sale Deed Doc No [number]/[year]".
    *   Format: [{{"document_name": "Sale Deed Doc No [number]/[year]", "document_date": "DD-MM-YYYY", "executed_by": "[Seller Name]", "in_favour_of": "[Buyer Name]", "nature_of_transfer": "Sale", "extent_owned": "[Area]"}}]


7. **MORTGAGE DOCUMENTS** (High Priority):
    *   Identify and extract ALL documents required or suggested for mortgage creation, exactly as listed in the document. These are documents that the lawyer recommends the company to obtain from the customer for the loan process.
    *   Look for sections with headings like "Documents", "Documents Required", "Original Documents", "List of Documents", "Documents submitted", "Documents scrutinized", "Documents verified", "Documents examined", "Documents to be collected", "Documents to be obtained", or similar phrases.
    *   Extract EVERY SINGLE DOCUMENT listed in these sections VERBATIM - do not skip any document, even if it seems redundant or was mentioned elsewhere.
    *   Pay special attention to phrases like "the following documents", "the company shall obtain", "the borrower shall submit", "required documents", etc.
    *   Include ALL details, descriptions, and notes provided for each document exactly as written.
    *   Look for numbered or bulleted lists of documents that appear after phrases like "The following documents are required" or "Documents to be collected".
    *   Look for documents mentioned in sections about "title verification", "legal requirements", or "documentation requirements".
    *   If a document to be collected is listed as Original list it as Original, if it is listed as Xerox or copy list it as Xerox or copy
    *   If you can determine whether a document is needed before loan disbursement (pre-disbursal) or after loan disbursement (post-disbursal), categorize it accordingly. Otherwise, place it in the pre_disbursal category by default.
    *   Format: {{"pre_disbursal": [{{"document_name": "document name with number", "document_description": "short description of that document" }}], "post_disbursal": [{{"document_name": "document name with number", "document_description": "description of the document" }}]}}
    *   CRITICAL: Do not omit ANY document mentioned in the relevant sections. Even if a document seems minor or redundant, include it in the list.
    *   VERIFY: After extraction, review the document again to ensure you haven't missed any documents. Check for documents that might be mentioned in paragraphs rather than in bullet points or numbered lists.

8.  **LEGAL OPINION (INITIAL & DETAILED)** (High Priority):
    *   **This section must be VERY DESCRIPTIVE and based ONLY on the document's text.**
    *   Provide a thorough, multi-sentence justification for each verdict. Do not use single-word answers.
    *   Format: {{
        "title_clear": {{
            "verdict": "Clear / Not Clear / Clear with Conditions",
            "detailed_justification": "Provide a comprehensive explanation for the title verdict. Reference specific documents, ownership links, and any breaks or concerns in the chain of title. Explain WHY the title is considered clear or not."
        }},
        "encumbrances": {{
            "verdict": "None Found / Encumbrances Found",
            "detailed_justification": "If encumbrances are found, list each one with its nature (e.g., mortgage, lien), amount, and the document reference. If none, state that the Encumbrance Certificate was checked for a specific period and found to be nil."
        }},
        "mortgage_viability": {{
            "verdict": "Suitable / Not Suitable / Suitable with Conditions",
            "detailed_justification": "Elaborate in detail on why the property is or is not suitable for creating a mortgage. Consider title clarity, property type, legal status, and any other relevant factors mentioned in the text."
        }},
        "risk_level": "Low / Medium / High",
        "recommendations": "Extract the specific, actionable recommendations made by the lawyer in the document.",
        "conclusion": "Provide a detailed summary of the lawyer's overall conclusion on the property's legal standing.",
        "lawyers_opinion_as_per_document": "Extract the verbatim legal opinion paragraph(s) from the document as a multi-line string.",
        "summarized_opinion": "Provide a neat, to-the-point summary of the lawyer's opinion in 2-4 sentences, incorporating key keywords from the document (e.g., 'clear title', 'encumbrance free', 'suitable for mortgage'), and ensuring it aligns directly with the verbatim opinion."
    }}

9.  **PROPERTY NATURE**:
    *   Format: {{"type": "Residential", "ownership_type": "Single", "access_type": "Direct Public Access", "special_conditions": "None", "classification": "Building"}}

10. **POTENTIAL ISSUES (FLAGS)**:
    *   Always thoroughly check for common risks in legal documents, including but not limited to: landlocked property (no access mentioned in boundaries), gaps in title history, missing documents, encumbrances not cleared, discrepancies in extent measurements, unclear ownership, etc.
    *   List all identified risks, discrepancies, or missing information.
    *   For each issue, include the related_section (one of: "property_address", "boundaries", "owner_names", "property_extent", "title_history", "documents_examined", "mortgage_documents", "legal_opinion", "property_nature").
    *   Format: [{{"issue_type": "Title Gap", "description": "Missing link document between [year1] and [year2].", "severity": "High", "evidence": "Quote from document or page number", "related_section": "title_history"}}]

**SPECIFIC FORMATTING INSTRUCTIONS:**
1.  For **TITLE HISTORY**:
    *   Use "executed_by" instead of "from".
    *   Use "in_favour_of" instead of "to".
    *   Omit the "owner" field in the final JSON structure for this key.
    *   For transfer nature: Use exact terms like "Sale", "Will", "Court Decree", "Gift", "Partition".

2.  For **PROPERTY EXTENT DETAILS**:
    *   Clearly separate "land_area" and "built_up_area".
    *   Include original units in the extracted string.

3.  For **OWNER NAME(S)**:
    *   Extract ONLY from sections explicitly labeled "Owner", "Title Owner".
    *   If a dedicated owner section is not found, return "Not Found".

Output only the single, complete JSON object. Do not add any text before or after the JSON.
"""
    return prompt

def fix_json_syntax(json_str):
    """Attempts to fix common JSON syntax errors."""
    print("Original JSON length:", len(json_str))
    
    # Replace single quotes with double quotes (if not within double-quoted strings)
    fixed = re.sub(r'(?<!"): ?\'([^\']*?)\'(?=[,}])', r': "\1"', json_str)
    
    # Fix missing commas between objects in arrays
    fixed = re.sub(r'}\s*{', '}, {', fixed)
    
    # Fix trailing commas in objects and arrays
    fixed = re.sub(r',\s*}', '}', fixed)
    fixed = re.sub(r',\s*\]', ']', fixed)
    
    # Fix missing quotes around property names
    fixed = re.sub(r'([{,])\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1 "\2":', fixed)
    
    # Fix escaped quotes within strings
    fixed = re.sub(r'\\+"', '"', fixed)
    
    # Handle truncated JSON (try to close unclosed brackets/braces)
    open_braces = fixed.count('{') - fixed.count('}')
    open_brackets = fixed.count('[') - fixed.count(']')
    
    if open_braces > 0 or open_brackets > 0:
        print(f"Detected unclosed structures: {open_braces} braces, {open_brackets} brackets")
        fixed += '}' * open_braces + ']' * open_brackets
    
    print("Fixed JSON length:", len(fixed))
    return fixed

def parse_single_doc_extraction_response_v2(response_text, filename, doc_text_content=""):
    """Parses LLM response and enhances address extraction with regex fallback."""
    try:
        # Add detailed logging
        print(f"Starting to parse response for {filename}")
        print(f"Response length: {len(response_text)} characters")
        
        # Extract JSON from response
        json_match = re.search(r'```json\s*({.*?})\s*```', response_text, re.DOTALL)
        if not json_match:
            # Log this specific case
            print("No JSON code block found in response")
            
            # Fallback if no code block is found, assume the entire response is JSON
            if response_text.strip().startswith('{') and response_text.strip().endswith('}'):
                print("Response appears to be a raw JSON object")
                json_str = response_text.strip()
            else:
                print("Response is not in JSON format")
                # Save the problematic response for debugging
                debug_file = f"debug_response_{datetime.now().strftime('%Y%m%d%H%M%S')}.txt"
                with open(debug_file, "w", encoding="utf-8") as f:
                    f.write(response_text)
                print(f"Saved problematic response to {debug_file}")
                return {"error": "No JSON object found in LLM response", "raw_response": response_text}
        else:
            print("Found JSON code block in response")
            json_str = json_match.group(1)

        # Try to parse the JSON
        try:
            print("Attempting to parse JSON")
            data = json.loads(json_str)
            print("JSON parsing successful")
        except json.JSONDecodeError as e:
            print(f"JSON parsing error: {e}")
            print(f"Error at position {e.pos}: {json_str[max(0, e.pos-20):min(len(json_str), e.pos+20)]}")
            
            # Try to fix common JSON issues
            print("Attempting to fix JSON syntax")
            fixed_json_str = fix_json_syntax(json_str)
            try:
                data = json.loads(fixed_json_str)
                print("Fixed JSON parsing successful")
            except json.JSONDecodeError as e2:
                print(f"Fixed JSON still has errors: {e2}")
                # Save the problematic JSON for debugging
                debug_file = f"debug_json_{datetime.now().strftime('%Y%m%d%H%M%S')}.json"
                with open(debug_file, "w", encoding="utf-8") as f:
                    f.write(json_str)
                print(f"Saved problematic JSON to {debug_file}")
                return {
                    "error": f"Failed to parse JSON response: {str(e)}",
                    "raw_response": response_text
                }
        data = json.loads(json_str)

        key_mapping = {
            "PROPERTY ADDRESS": "property_address",
            "BOUNDARIES": "boundaries",
            "OWNER NAME(S)": "owner_names",
            "PROPERTY EXTENT DETAILS": "property_extent",
            "TITLE HISTORY": "title_history",
            "DOCUMENTS EXAMINED": "documents_examined",
            "MORTGAGE DOCUMENTS": "mortgage_documents",
            "LEGAL OPINION": "legal_opinion",
            "PROPERTY NATURE": "property_nature",
            "POTENTIAL ISSUES (FLAGS)": "potential_issues",
            "GUIDANCE QUESTIONS": "guidance_questions" # Updated key
        }

        standardized_data = {}

        # Lowercase all keys in the parsed data for case-insensitive matching
        data_lower = {k.lower().replace(" ", "_").replace("(", "").replace(")", "").replace("-", "_"): v for k, v in data.items()}

        for aiprompt_key, python_key in key_mapping.items():
            # Handle the old and new key for questions for backward compatibility
            lookup_key = aiprompt_key.lower().replace(" ", "_").replace("(", "").replace(")", "").replace("-", "_")
            if lookup_key in data_lower:
                standardized_data[python_key] = data_lower[lookup_key]
            # Specific fallback check for the old "regulatory_questions" key name
            elif python_key == "guidance_questions" and "regulatory_questions" in data_lower:
                 standardized_data[python_key] = data_lower["regulatory_questions"]
            else:
                standardized_data[python_key] = "Not Found"


        # Enhanced address extraction fallback
        if not standardized_data.get("property_address") or str(standardized_data.get("property_address", "")).lower() in ["not found", "n/a"]:
            # First, try the user's specific pattern for explicit full address
            full_address = extract_full_address_line(doc_text_content)
            if full_address:
                 standardized_data["property_address"] = full_address  # Extract as is
            else:
                # If that fails, try the component-based approach and format as specified
                address_components = extract_address_components(doc_text_content)
                if address_components:
                    constructed_address = construct_address(address_components)
                    standardized_data["property_address"] = constructed_address
                else:
                    # Final fallback to paragraph with most keywords
                    paragraphs = doc_text_content.split('\n\n')
                    best_paragraph = None
                    max_keywords = 0
                    for para in paragraphs:
                        para_lower = para.lower()
                        keyword_count = sum(1 for keyword in ADDRESS_KEYWORDS if keyword in para_lower)
                        if keyword_count > max_keywords:
                            max_keywords = keyword_count
                            best_paragraph = para
                    if best_paragraph:
                        standardized_data["property_address"] = best_paragraph.strip()
                    else:
                        standardized_data["property_address"] = "Not Found"


        # (New) Add owner name extraction fallback from document text
        if not standardized_data.get("owner_names") or standardized_data.get("owner_names") == "Not Found":
            owner_names = extract_owner_names(doc_text_content)
            if owner_names:
                standardized_data["owner_names"] = owner_names

        # (New) Process property extent with fallback
        if not standardized_data.get("property_extent") or standardized_data.get("property_extent") == "Not Found":
            standardized_data["property_extent"] = extract_property_extent(doc_text_content)


        # Validate mortgage document structure
        mortgage_docs = standardized_data.get("mortgage_documents", {})
        if isinstance(mortgage_docs, dict):
            if not isinstance(mortgage_docs.get("pre_disbursal"), list):
                mortgage_docs["pre_disbursal"] = []
            if not isinstance(mortgage_docs.get("post_disbursal"), list):
                mortgage_docs["post_disbursal"] = []
            standardized_data["mortgage_documents"] = mortgage_docs
        else:
            # If the format is wrong, reset it to the expected structure
            standardized_data["mortgage_documents"] = {"pre_disbursal": [], "post_disbursal": []}

        # Standardize and convert property extent details
        if "property_extent" in standardized_data and isinstance(standardized_data["property_extent"], dict):
            extent = standardized_data["property_extent"]

            # Land Area
            land_area_orig = extent.get("land_area", "Not Found")
            converted_land_area, _ = convert_to_sqft(land_area_orig)
            if converted_land_area:
                extent["land_area_sqft"] = converted_land_area

            # Built-up Area
            built_up_area_orig = extent.get("built_up_area", "Not Found")
            converted_built_up_area, _ = convert_to_sqft(built_up_area_orig)
            if converted_built_up_area:
                extent["built_up_area_sqft"] = converted_built_up_area

        # Enhanced property classification
        if "property_nature" in standardized_data and isinstance(standardized_data["property_nature"], dict):
            nature = standardized_data["property_nature"]
            boundaries_list = standardized_data.get("boundaries", [])

            # Ensure potential_issues is a list
            if "potential_issues" not in standardized_data or not isinstance(standardized_data["potential_issues"], list):
                standardized_data["potential_issues"] = []

            # Per-set landlocked check
            landlocked_sets = []
            has_access = False
            for idx, boundary in enumerate(boundaries_list):
                boundary_text = ' '.join([str(v).lower() for v in boundary.values()])
                access_present = any(kw in boundary_text for kw in ACCESS_KEYWORDS)
                if access_present:
                    has_access = True
                if not access_present:
                    landlocked_sets.append(idx + 1)
                    landlocked_issue = {
                        "issue_type": "Landlocked Property",
                        "description": f"S.No {idx + 1} is a landlocked property based on boundary descriptions.",
                        "severity": "High",
                        "evidence": "No access-related keywords found in this set of boundaries.",
                        "related_section": "boundaries"
                    }
                    # Check if already flagged to avoid duplicates
                    if not any(issue.get("description") == landlocked_issue["description"] for issue in standardized_data["potential_issues"]):
                        standardized_data["potential_issues"].append(landlocked_issue)

            # Determine overall access type
            if len(boundaries_list) > 0:
                if landlocked_sets == []:
                    nature["access_type"] = "Direct Public Access"
                elif len(landlocked_sets) == len(boundaries_list):
                    nature["access_type"] = "Landlocked"
                else:
                    nature["access_type"] = "Mixed Access"

            # Classify property type based on indicators in the full document text
            building_indicators = ["floor", "storey", "building", "constructed", "sq.ft", "sq ft", "built-up"]
            building_mentioned = any(indicator in doc_text_content.lower() for indicator in building_indicators)
            nature["classification"] = "Building" if building_mentioned else "Plot"

        # (New) Enhanced title history parsing
        title_history_raw = standardized_data.get("title_history")
        history_as_list = []

        if isinstance(title_history_raw, list):
            history_as_list = title_history_raw
        elif isinstance(title_history_raw, dict):
            # Convert dictionary of histories into a list
            history_as_list = [v for k, v in title_history_raw.items() if isinstance(v, dict)]

        if history_as_list:
            # Use the new processing function
            processed_history = process_title_history(history_as_list)
            # Add sequence numbers back in
            final_history = []
            for i, entry in enumerate(processed_history):
                final_history.append({"sequence": i + 1, **entry})
            standardized_data["title_history"] = final_history
        else:
            # If no history, ensure it's an empty list for consistency
            standardized_data["title_history"] = []

        # Enhanced legal opinion handling
        if "legal_opinion" in standardized_data and isinstance(standardized_data["legal_opinion"], dict):
            # Get verbatim opinion using the improved extractor
            opinion_from_doc = extract_legal_opinion(doc_text_content)
            
            # Preserve exact formatting and content
            standardized_data["legal_opinion"]["lawyers_opinion_as_per_document"] = opinion_from_doc

        # Ensure boundaries are parsed into a list of dicts for multiple sets
        boundaries_raw = standardized_data.get("boundaries", "Not Found")
        boundaries_list = []
        if isinstance(boundaries_raw, list):
            for b in boundaries_raw:
                if isinstance(b, dict):
                    boundary_dict = {k.lower(): v for k, v in b.items()}
                    boundaries_list.append(boundary_dict)
        elif isinstance(boundaries_raw, dict):
            boundary_dict = {k.lower(): v for k, v in boundaries_raw.items()}
            boundaries_list = [boundary_dict]
        elif isinstance(boundaries_raw, str) and boundaries_raw != "Not Found":
            # Parse string fallback for single set
            boundary_dict = {}
            parts = [p.strip() for p in boundaries_raw.split(',') if p.strip()]
            for part in parts:
                if ':' in part:
                    key, val = part.split(':', 1)
                    key_clean = key.strip().lower()
                    if key_clean in ['north', 'south', 'east', 'west']:
                        boundary_dict[key_clean] = val.strip()
            if boundary_dict:
                boundaries_list = [boundary_dict]
        if not boundaries_list:
            boundaries_list = [{"north": "Not Found", "south": "Not Found", "east": "Not Found", "west": "Not Found"}]
        standardized_data["boundaries"] = boundaries_list

        return standardized_data

    except Exception as e:
        return {"error": f"Parsing failed: {str(e)}", "raw_response": response_text}

def create_final_opinion_prompt(extracted_data_json: str) -> str:
    """
    Creates the second prompt for the LLM. This prompt includes the extracted data
    and a hardcoded set of rules from the user's CSV to generate a final opinion.
    """

    # The rules are taken directly from the user's CSV.
    hardcoded_rules = """
    **INTERNAL GUIDELINES FOR LEGAL OPINION:**

    Based on the extracted data, you must analyze the following sections and provide specific guidance if any of these conditions are met. Your output should be a final, grounded legal opinion that includes these observations.

    **1. Address Section Rules:**
    - If the address is missing a Door No./Plot No., your guidance must be: "Door No./Plot No. is missing. Pls validate with technical report, property tax, utility bill."
    - If the address is missing a Survey No., your guidance must be: "Survey no. is missing. Pls validate with technical report, property tax".
    - If the address is missing a Street name/Locality, your guidance must be: "Street name/Locality missing".
    - If the address is missing the City, your guidance must be: "City not mentioned?".
    - If the address is missing the State, your guidance must be: "State not mentioned?".
    - If the address is missing the Pincode, your guidance must be: "Pincode not mentioned?".
    - The address should not contain special characters like #, ?, % or words like "NULL", "NA". If they exist, highlight this as a remark.

    **2. Boundaries Section Rules:**
    - If boundary details are missing entirely, highlight this. No NULL values, "NA" or specical characters allowed.
    - If not even one side of the property has access through a public road (check for keywords like 'road', 'street', 'path', etc.), your guidance must be: "Min one side access to the property not available. Pls document survey report to confirm access to the property".
    - If there are more than 4 boundaries mentioned, highlight this as unusual.

    **3. Owner Name/s Section Rules:**
    - If the property owner name is not mentioned, your guidance must be: "Property owner name is mandatory".
    - If the property is owned by a Ltd company, your guidance must include: "Pls ensure to create ROC charge post disbursement".

    **4. Extent Details Section Rules:**
    - If the extent of land area is not mentioned, highlight that "Extent not mentioned in legal report".
    - The area must be mentioned in sq ft. If not, convert it and state the converted value.
    - You must ask: "Has the minimum area as per policy met?".

    **5. Documents Examined Section Rules:**
    - You must instruct: "Documents to be arranged basis date in chronological order from old to latest".
    - If an Encumbrance Certificate (EC) is listed, you must instruct: "In case of EC (Encumbrance certificate) date range of the same to be mentioned".

    **6. Title History/Title Flow Section Rules:**
    - If a 'Will deed' is part of the title flow, you must ask: "Is the will probated? If Will is noted in last 13 years, please exercise caution and take necessary approval".
    - If the latest title deed is a 'partition deed', you must ask: "Latest title deed is partition deed. Is original provided to CIFCL? If not and take necessary approval".
    - If another 'partition deed' is mentioned as a parent document, you must ask: "Partition deed observed as parent deed in title flow. Whether original is obtained?. If not and take necessary approval".
    - If any 'court decree/litigation' is observed, your guidance is: "Pls exercise caution and take necessary approval".
    - If a 'WAQF board allotment deed' is observed, you must ask: "Is it WAQF board alloted property? If yes, and take necessary approval".
    - If any 'lease deed' is observed, you must ask: "1. Is it leasehold property? 2. Is it subleased to a third party? 3. Whether the lease is given by govt body or private/individual?. If private, its not allowed as per policy".
    - If the Encumbrance Certificate search date is >30 days older than the legal report date, your guidance is: "Latest EC to be documented".
    - If any active mortgage transactions are not cancelled, your guidance is: "1. Active mortgage charge found that is not yet satisfied. Pls ensure satisfaction of earlier mortgage before disbursement. 2. LOD to be documented in case of BT transactions mandatorily".
    - If the property tax receipt is older than 180 days, your guidance is: "Pls document latest property tax reciept".
    - If the property was purchased through 'SARFAESI/auction/sale certificate', your guidance is: "Is the property purchased through SARFAESI/sale certificate/auction?. If yes pls exercise caution and take necessary approval".
    - If an 'unregistered title deed' is observed, your guidance is: "An unregistered deed is not acceptable as per policy".
    - If the latest property owner name mismatches the name declared earlier, you must ask: "Is there a mismatch in owner name as per title flow. In case of purchase case ensure sale agreement is documented, OCR is paid and cheque hand over is happening only against registration?".
    - If any title deeds are 'lost/FIR filed/CTC copy' observed, you must ask: "Whether all loss of documents formalities fully completed? Pls excerice caution and take necessary approval".
    - If any title deed is dated <180 days from the legal report date, your guidance is: "One or more title deed is dated <180 days from the date of legal report. Pls exercise caution and take necessary approvals".

    **7. Mortgage Documents Section Rules:**
    - If not all documents from the title flow are listed here, you must state: "Title deeds missing as per title flow (to be highlighted)".
    - If the latest title deed is a photocopy, your guidance is: "Latest title deed to be obtained in original only".
    - If MODT cancellation from a BT banker is not mentioned (when applicable), you must ask: "Is the existing mortgage to be satisfied before disbursement?".

    **8. Legal Opinion Section Rules:**
    - The final opinion must mandatorily contain the phrase "Clear and marketable". If not, your guidance is: "'Clear and marketable title' certificate to be obtained in the final legal opinion".
    - If the owner name in the final opinion certificate mismatches the title flow, your guidance is: "Owner name mismatch as per title flow. Pls cross check with title flow".
    """

    prompt = f"""You are a Chief Legal Officer AI. You will be given a JSON object containing data extracted from a legal opinion document. Your task is to analyze this data against a set of internal guidelines and produce a final, grounded legal opinion.

**EXTRACTED DOCUMENT DATA:**
```json
{extracted_data_json}
```

{hardcoded_rules}

**YOUR FINAL TASK:**
Based on your analysis of the data against the guidelines, generate a comprehensive "Grounded Legal Opinion". This opinion should be a well-structured report.
First, create a section for each of the main categories (Address, Boundaries, Owner Name/s, Extent details, Documents Examined, title history/title flow, mortage documents, legal opinion).
Under each section, list all the guidance points or observations you have identified based on the rules. If no issues are found for a section, state "No issues observed."
Finally, provide a "Final Grounded Legal Opinion" section which is a concise, executive-level summary paragraph that concludes with a clear recommendation (e.g., "Approve", "Approve with Conditions", "Reject").

**OUTPUT FORMAT:**
Provide the output as a single, clean markdown text. Do not wrap it in JSON or code blocks.
Example structure:

**Address**
- Pincode not mentioned?
- City not mentioned?

**Boundaries**
- No issues observed.

**Title History/Title Flow**
- Is the will probated? If Will is noted in last 13 years, please exercise caution and take necessary approval.

... (and so on for all 8 sections)

**Final Grounded Legal Opinion**
The property title appears clear, however, several key details are missing from the address including the pincode and city. Furthermore, a will deed in the title flow requires probation confirmation. Based on these observations, the recommendation is **Approve with Conditions**: all missing information must be provided and validated, and the will must be confirmed as probated before disbursement.
"""
    return prompt

def validate_extraction_result(data):
    """Validates that the extraction result has the expected structure."""
    if not isinstance(data, dict):
        return False, f"Result is not a dictionary: {type(data)}"
    
    # Check for required keys
    required_keys = [
        "PROPERTY ADDRESS",
        "BOUNDARIES",
        "OWNER NAME(S)",
        "PROPERTY EXTENT DETAILS",
        "DOCUMENTS EXAMINED",
        "TITLE HISTORY (CHRONOLOGICAL ORDER)",
        "MORTGAGE DOCUMENTS",
        "LEGAL OPINION (INITIAL & DETAILED)",
        "PROPERTY NATURE"
    ]
    
    missing_keys = [key for key in required_keys if key not in data]
    
    if missing_keys:
        return False, f"Missing required keys: {', '.join(missing_keys)}"
    
    return True, ""

def render_single_doc_view():
    st.header("Legal Document Analysis")

    if "single_doc_result_v2" not in st.session_state:
        st.session_state.single_doc_result_v2 = None

    # This state is no longer populated by RAG but kept for UI structure consistency
    if "enhanced_doc_result" not in st.session_state:
        st.session_state.enhanced_doc_result = None

    if "final_grounded_opinion" not in st.session_state:
        st.session_state.final_grounded_opinion = None
        
    # RAG lookups are no longer generated
    if "rag_lookups" not in st.session_state:
        st.session_state.rag_lookups = []


    uploaded_file = st.file_uploader(
        "Upload legal document (PDF/TXT)",
        type=["pdf", "txt"],
        key="single_doc_uploader_v2"
    )

    if uploaded_file and st.button("üîé Analyze Document", use_container_width=True):
        # Clear previous results
        st.session_state.single_doc_result_v2 = None
        st.session_state.enhanced_doc_result = None
        st.session_state.final_grounded_opinion = None
        st.session_state.rag_lookups = []

        with st.status("Processing...", expanded=True) as status:
            # Save uploaded file to temporary path
            with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as temp_file:
                temp_file.write(uploaded_file.getvalue())
                temp_path = temp_file.name

            try:
                # --- STEP 1: First Gemini call for Data Extraction ---
                status.update(label="Performing initial data extraction with AI...")
                prompt = create_single_doc_extraction_prompt(uploaded_file.name)
                response = process_document(temp_path, prompt)
                
                # Clean up temp file
                try:
                    os.unlink(temp_path)
                except Exception as cleanup_error:
                    print(f"Warning: Could not delete temp file: {cleanup_error}")

                if response.get("error"):
                    status.update(label="AI data extraction failed", state="error")
                    st.session_state.single_doc_result_v2 = {"error": response["error"]}
                    return

                # Use the structured_data directly if available
                if "structured_data" in response and response["structured_data"]:
                    extracted_data = response["structured_data"]
                else:
                    status.update(label="AI analysis failed to return structured data", state="error")
                    st.session_state.single_doc_result_v2 = {"error": "The AI model failed to return a valid structured response."}
                    return
                
                # Store initial extraction result
                st.session_state.single_doc_result_v2 = extracted_data
                st.session_state.enhanced_doc_result = extracted_data # For UI compatibility
                
                # --- STEP 2: Second Gemini call for Grounded Opinion ---
                status.update(label="Generating guidance-grounded legal opinion...")

                # Convert extracted data dict to a JSON string for the next prompt
                extracted_data_json_str = json.dumps(extracted_data, indent=2)

                # Create the final prompt with hardcoded rules
                final_prompt = create_final_opinion_prompt(extracted_data_json_str)

                # Create a new client for the text generation call
                os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = config.CREDENTIALS_PATH
                client = genai.Client(vertexai=True, project=config.VERTEX_AI_PROJECT_ID, location=config.VERTEX_AI_LOCATION)
                model = config.VERTEX_AI_MODEL

                final_opinion = generate_text_content(client, model, final_prompt)
                
                if final_opinion:
                    st.session_state.final_grounded_opinion = final_opinion
                else:
                    # Handle case where final opinion generation fails
                    st.warning("Could not generate the final grounded opinion.")

                status.update(label="Analysis complete!", state="complete")

            except Exception as e:
                print(f"Error during document analysis: {e}")
                import traceback
                traceback.print_exc()
                status.update(label=f"Analysis failed: {str(e)}", state="error")
                st.session_state.single_doc_result_v2 = {"error": f"Analysis exception: {str(e)}"}
                return
                
        st.rerun()

    # Display results
    if st.session_state.single_doc_result_v2:
        result = st.session_state.single_doc_result_v2
        enhanced_result = st.session_state.enhanced_doc_result or result
        # RAG lookups are now an empty list
        rag_lookups = st.session_state.rag_lookups or []

        # More robust error checking
        has_error = False
        error_message = ""
        
        if not isinstance(result, dict):
            has_error = True
            error_message = f"Result is not a dictionary: {type(result)}"
        elif "error" in result and result["error"]:
            has_error = True
            error_message = result["error"]
        
        if has_error:
            st.error(f"Analysis error: {error_message}")
            if isinstance(result, dict) and "raw_response" in result:
                with st.expander("Show Raw AI Response"):
                    st.code(result["raw_response"])
            return

        st.subheader(f"Legal Analysis Report for: {uploaded_file.name}")

        # --- RESTRUCTURED UI SECTIONS ---
        
        # This helper function is now empty as RAG is removed, but kept for UI code stability
        def display_rag_lookups(lookups):
            """Display RAG lookups in a consistent format - now does nothing."""
            return

        # 1. Property Summary
        with st.expander("Property Summary", expanded=True):
            cols = st.columns(3)
            cols.write("**Address:**")
            cols.info(result.get("PROPERTY ADDRESS", "Not Found"))
            cols.write("**Owner(s):**")
            cols.info(result.get("OWNER NAME(S)", "Not Found"))

            cols.write("**Property Extent:**")
            extent_data = result.get("PROPERTY EXTENT DETAILS", {})
            if isinstance(extent_data, dict):
                land_area_orig = extent_data.get("land_area", "Not Found")
                land_area_sqft = extent_data.get("land_area_sqft")
                built_up_area_orig = extent_data.get("built_up_area", "Not Found")
                built_up_area_sqft = extent_data.get("built_up_area_sqft")
                floor_wise_area = extent_data.get("floor_wise_areas", "Not Found")

                display_text_list = []
                if land_area_sqft:
                    display_text_list.append(f"**Land:** {land_area_orig} ({land_area_sqft})")
                else:
                    display_text_list.append(f"**Land:** {land_area_orig}")

                if built_up_area_sqft:
                    display_text_list.append(f"**Built-up:** {built_up_area_orig} ({built_up_area_sqft})")
                else:
                    if built_up_area_orig.lower() not in ["not found", ""]:
                        display_text_list.append(f"**Built-up:** {built_up_area_orig}")

                if floor_wise_area.lower() not in ["not found", ""]:
                    display_text_list.append(f"**Floor-wise:** {floor_wise_area}")

                cols.info("\n\n".join(display_text_list))
            else:
                cols.info(str(extent_data))

            st.write("**Boundaries:**")
            boundaries_data = result.get("BOUNDARIES", [])
            if isinstance(boundaries_data, list) and boundaries_data:
                # Display as table with multiple rows if multiple sets
                boundary_rows = []
                for i, b in enumerate(boundaries_data):
                    boundary_rows.append({
                        "S.No": i + 1,
                        "North": b.get("north", "Not Found"),
                        "South": b.get("south", "Not Found"),
                        "East": b.get("east", "Not Found"),
                        "West": b.get("west", "Not Found")
                    })
                boundary_df = pd.DataFrame(boundary_rows)
                st.dataframe(boundary_df, use_container_width=True, hide_index=True)
            else:
                st.info("Not Found")
            
            # Discrepancies for this section
            section_issues = [issue for issue in result.get("POTENTIAL ISSUES (FLAGS)", [])
                             if isinstance(issue, dict) and issue.get("related_section") in ["boundaries", "BOUNDARIES"]]
            if section_issues:
                st.subheader("Discrepancies")
                df_discrepancies = pd.DataFrame(section_issues)
                st.dataframe(df_discrepancies, use_container_width=True, hide_index=True)
            else:
                st.success("No discrepancies found in initial extraction.")

        # 4. Documents Examined (Moved before Title History)
        with st.expander("Documents Examined", expanded=False):
            docs_examined = result.get("DOCUMENTS EXAMINED", [])

            # No filtering, no deduplication ‚Äî show all documents exactly as extracted
            if isinstance(docs_examined, list) and docs_examined:
                st.dataframe(pd.DataFrame(docs_examined), use_container_width=True, hide_index=True)
            else:
                st.warning("No examined document details found or format is incorrect.")
                
            # Discrepancies for this section
            section_issues = [issue for issue in result.get("POTENTIAL ISSUES (FLAGS)", [])
                             if isinstance(issue, dict) and issue.get("related_section") in ["documents_examined", "DOCUMENTS EXAMINED"]]
            if section_issues:
                st.subheader("Discrepancies")
                df_discrepancies = pd.DataFrame(section_issues)
                st.dataframe(df_discrepancies, use_container_width=True, hide_index=True)
            else:
                st.success("No discrepancies found in initial extraction.")

        # 2. Title History (Updated)
        with st.expander("üìú Title History", expanded=True):
            title_history = result.get("TITLE HISTORY (CHRONOLOGICAL ORDER)", [])
            if isinstance(title_history, list) and title_history:
                df_history = pd.DataFrame(title_history)
                # Updated column order as per request
                column_order = ['sequence', 'document_date', 'document_name','executed_by', 'in_favour_of', 'nature_of_transfer', 'extent_owned']
                df_history_ordered = df_history.reindex(columns=[col for col in column_order if col in df_history.columns])
                st.dataframe(df_history_ordered, use_container_width=True, hide_index=True)

                st.subheader("TITLE FLOW")
                for entry in title_history:
                    with st.container(border=True):
                        cols = st.columns()
                        cols.write(f"**#{entry.get('sequence', '')}**")
                        cols.write(f"**Date:** {entry.get('document_date', '')}")
                        cols.write(f"**Document:** {entry.get('document_name', '')}")
                        cols.write(f"**Executed By:** {entry.get('executed_by', '')}")
                        cols.write(f"**In Favour of:** {entry.get('in_favour_of', '')}")
            else:
                st.warning("No title history found or format is incorrect.")

            # Discrepancies for this section
            section_issues = [issue for issue in result.get("POTENTIAL ISSUES (FLAGS)", [])
                             if isinstance(issue, dict) and issue.get("related_section") in ["title_history", "TITLE HISTORY", "TITLE HISTORY (CHRONOLOGICAL ORDER)"]]
            if section_issues:
                st.subheader("Discrepancies")
                df_discrepancies = pd.DataFrame(section_issues)
                st.dataframe(df_discrepancies, use_container_width=True, hide_index=True)
            else:
                st.success("No discrepancies found in initial extraction.")

        # 3. Property Nature
        with st.expander("Property Nature", expanded=False):
            nature = result.get("PROPERTY NATURE", {})
            if isinstance(nature, dict):
                st.write("**Property Classification**")
                st.info(nature.get("classification", "N/A"))

                cols = st.columns(4)
                with cols:
                    st.write("**Type**")
                    st.info(nature.get("type", "N/A"))
                with cols:
                    st.write("**Ownership**")
                    st.info(nature.get("ownership_type", "N/A"))
                with cols:
                    st.write("**Access**")
                    st.info(nature.get("access_type", "N/A"))
                with cols:
                    st.write("**Conditions**")
                    st.info(nature.get("special_conditions", "N/A"))
            else:
                st.warning("Property nature details not found or in an incorrect format.")

            # Discrepancies for this section
            section_issues = [issue for issue in result.get("POTENTIAL ISSUES (FLAGS)", [])
                             if isinstance(issue, dict) and issue.get("related_section") in ["property_nature", "PROPERTY NATURE"]]
            if section_issues:
                st.subheader("Discrepancies")
                df_discrepancies = pd.DataFrame(section_issues)
                st.dataframe(df_discrepancies, use_container_width=True, hide_index=True)
            else:
                st.success("No discrepancies found in initial extraction.")

        # 5. Mortgage Documents
        with st.expander("Mortgage Documents", expanded=False):
            mortgage_docs = result.get("MORTGAGE DOCUMENTS", {})
            
            pre_disbursal = mortgage_docs.get("pre_disbursal", [])
            post_disbursal = mortgage_docs.get("post_disbursal", [])
            
            if not pre_disbursal and not post_disbursal:
                st.warning("No mortgage documents found in the document.")
            else:
                if not post_disbursal:
                    st.write("**Required Documents**")
                    if pre_disbursal:
                        df_docs = pd.DataFrame(pre_disbursal)
                        st.dataframe(df_docs, use_container_width=True, hide_index=True)
                else:
                    st.write("**Pre-Disbursal Documents**")
                    if pre_disbursal:
                        df_pre = pd.DataFrame(pre_disbursal)
                        st.dataframe(df_pre, use_container_width=True, hide_index=True)
                    else:
                        st.info("No pre-disbursal documents specified.")

                    st.write("**Post-Disbursal Documents**")
                    df_post = pd.DataFrame(post_disbursal)
                    st.dataframe(df_post, use_container_width=True, hide_index=True)

            # Discrepancies for this section
            section_issues = [issue for issue in result.get("POTENTIAL ISSUES (FLAGS)", [])
                             if isinstance(issue, dict) and issue.get("related_section") in ["mortgage_documents", "MORTGAGE DOCUMENTS"]]
            if section_issues:
                st.subheader("Discrepancies")
                df_discrepancies = pd.DataFrame(section_issues)
                st.dataframe(df_discrepancies, use_container_width=True, hide_index=True)
            else:
                st.success("No discrepancies found in initial extraction.")

        # 6. Initial Legal Opinion
        with st.expander("Initial Legal Opinion (Detailed View)", expanded=True):
            opinion = result.get("LEGAL OPINION (INITIAL & DETAILED)", {})
            if isinstance(opinion, dict):
                metrics = {
                    "title_clear": ("Title Status", "detailed_justification"),
                    "encumbrances": ("Encumbrances", "detailed_justification"),
                    "mortgage_viability": ("Mortgage Viability", "detailed_justification")
                }
                for key, (title, just_key) in metrics.items():
                    st.markdown(f"**{title}**")
                    metric_data = opinion.get(key, {})
                    if isinstance(metric_data, dict):
                        verdict = metric_data.get("verdict", "N/A")
                        justification = metric_data.get(just_key, "No detailed justification provided.")
                        col1, col2 = st.columns()
                        with col1:
                            st.write("**Verdict**")
                            st.info(f"{verdict}")
                        with col2:
                            st.write("**Detailed Justification**")
                            st.info(justification)
                    else: st.warning(f"Data for '{title}' is not in the expected format.")
                    st.markdown("---")

                st.write("**Recommendations**")
                st.success(f'**Expert Recommendation:** {opinion.get("recommendations", "No recommendations provided")}')
                st.write("**Expert Conclusion**")
                st.info(f'**Expert Conclusion:** {opinion.get("conclusion", "No conclusion provided")}')

                if "summarized_opinion" in opinion:
                    st.write("**Summarized Lawyer's Opinion**")
                    st.info(f'{opinion.get("summarized_opinion", "No summarized opinion found")}')

            else: st.error("Legal opinion section is missing or in an incorrect format.")

            # Discrepancies for this section
            section_issues = [issue for issue in result.get("POTENTIAL ISSUES (FLAGS)", [])
                             if isinstance(issue, dict) and issue.get("related_section") in ["legal_opinion", "LEGAL OPINION", "LEGAL OPINION (INITIAL & DETAILED)"]]
            if section_issues:
                st.subheader("Discrepancies")
                df_discrepancies = pd.DataFrame(section_issues)
                st.dataframe(df_discrepancies, use_container_width=True, hide_index=True)
            else:
                st.success("No discrepancies found in initial extraction.")

        # 7. Potential Issues (Flags)
        with st.expander("üö® Potential Issues (Flags) from Initial Extraction", expanded=True):
            issues = result.get("POTENTIAL ISSUES (FLAGS)", [])
            if isinstance(issues, list) and issues:
                df_issues = pd.DataFrame(issues)
                def highlight_severity(s):
                    color = 'gray'
                    s_lower = str(s).lower()
                    if 'high' in s_lower: color = '#FF4B4B'
                    elif 'medium' in s_lower: color = '#FFC400'
                    elif 'low' in s_lower: color = '#22A762'
                    return f'color: {color}; font-weight: bold;'
                st.dataframe(df_issues.style.applymap(highlight_severity, subset=['severity']), use_container_width=True, hide_index=True)
            else: st.success("‚úÖ No potential issues flagged by the AI in the initial extraction.")

        # 8. Final Grounded Opinion
        if st.session_state.final_grounded_opinion:
            with st.expander("üîç Final Guidance-Grounded Opinion", expanded=True):
                st.markdown("### Final Legal Opinion (Based on Hardcoded Guidelines)")
                st.markdown(st.session_state.final_grounded_opinion)

        # Download report
        st.markdown("---")
        report_data = enhanced_result
        if st.session_state.final_grounded_opinion:
            if isinstance(report_data, dict):
                report_data["final_grounded_opinion"] = st.session_state.final_grounded_opinion

        pdf_data = generate_legal_report_pdf(uploaded_file.name, report_data)
        st.download_button("üì• Download Full Legal Report", pdf_data,
                          file_name=f"legal_analysis_{Path(uploaded_file.name).stem}.pdf",
                          mime="application/pdf", use_container_width=True)

        # Add a debug section to view the raw data
        with st.expander("Debug: View Raw Extracted Data Structure", expanded=False):
            st.json(result)

def generate_legal_report_pdf(filename, analysis_data, rag_lookups=None):
    buffer = io.BytesIO()
    # Use landscape orientation for more horizontal space
    doc_template = SimpleDocTemplate(buffer, pagesize=landscape(letter), title=f"Legal Analysis: {filename}",
                            leftMargin=0.25*inch, rightMargin=0.25*inch,
                            topMargin=0.25*inch, bottomMargin=0.25*inch)

    styles = getSampleStyleSheet()
    # Use smaller fonts throughout
    styles['Title'].fontSize = 9
    styles['Heading2'].fontSize = 8
    styles['Heading3'].fontSize = 7
    styles['Normal'].fontSize = 6
    styles.add(ParagraphStyle(name='Justify', alignment=4, fontSize=7))
    styles.add(ParagraphStyle(name='SmallNormal', parent=styles['Normal'], fontSize=6))
    styles.add(ParagraphStyle(name='Tiny', parent=styles['Normal'], fontSize=5))

    elements = []

    # Title and header
    elements.append(Paragraph(f"LEGAL ANALYSIS REPORT: {filename}", styles['Title']))

    # --- FINAL GROUNDED OPINION (Display at the top) ---
    if "final_grounded_opinion" in analysis_data and analysis_data["final_grounded_opinion"]:
        elements.append(Spacer(1, 10))
        elements.append(Paragraph("Final Guidance-Grounded Opinion", styles['Heading2']))
        opinion_text = analysis_data["final_grounded_opinion"].replace('\n', '<br/>')
        elements.append(Paragraph(opinion_text, styles['SmallNormal']))
        elements.append(Spacer(1, 10))

    # We'll create content for left and right columns separately
    
    # --- LEFT COLUMN CONTENT ---
    left_column = []
    
    # Property Summary
    left_column.append(Paragraph("Property Summary", styles['Heading2']))
    
    # Format property extent
    extent_data = analysis_data.get("PROPERTY EXTENT DETAILS", {})
    extent_str = ""
    if isinstance(extent_data, dict):
        land_area = extent_data.get("land_area", "Not Found")
        land_area_sqft = extent_data.get("land_area_sqft", "")
        built_up = extent_data.get("built_up_area", "Not Found")
        built_up_sqft = extent_data.get("built_up_area_sqft", "")
        
        extent_str = f"Land: {land_area}"
        if land_area_sqft: extent_str += f" ({land_area_sqft})"
        extent_str += f"<br/>Built-up: {built_up}"
        if built_up_sqft: extent_str += f" ({built_up_sqft})"
    else:
        extent_str = str(extent_data)
    
    summary_data = [
        [Paragraph("<b>Address:</b>", styles['SmallNormal']),
         Paragraph(str(analysis_data.get("PROPERTY ADDRESS", "Not Found")), styles['SmallNormal'])],
        [Paragraph("<b>Owners:</b>", styles['SmallNormal']),
         Paragraph(str(analysis_data.get("OWNER NAME(S)", "Not Found")), styles['SmallNormal'])],
        [Paragraph("<b>Extent:</b>", styles['SmallNormal']),
         Paragraph(extent_str, styles['SmallNormal'])]
    ]
    
    if "PROPERTY NATURE" in analysis_data and isinstance(analysis_data.get("PROPERTY NATURE"), dict):
        prop_nature = analysis_data["PROPERTY NATURE"]
        nature_str = f"Class: {prop_nature.get('classification', 'Not Found')}<br/>"
        nature_str += f"Type: {prop_nature.get('type', 'Not Found')}<br/>"
        nature_str += f"Ownership: {prop_nature.get('ownership_type', 'Not Found')}<br/>"
        nature_str += f"Access: {prop_nature.get('access_type', 'Not Specified')}"
        
        summary_data.append([
            Paragraph("<b>Nature:</b>", styles['SmallNormal']),
            Paragraph(nature_str, styles['SmallNormal'])
        ])
    
    summary_table = Table(summary_data, colWidths=[0.8*inch, 4.2*inch])
    summary_table.setStyle(TableStyle([
        ('GRID', (0,0), (-1,-1), 0.5, colors.grey),
        ('VALIGN', (0,0), (-1,-1), 'TOP')
    ]))
    left_column.append(summary_table)
    
    # Boundaries
    boundaries_data = analysis_data.get("BOUNDARIES", [])
    if isinstance(boundaries_data, list) and boundaries_data:
        left_column.append(Spacer(1, 5))
        left_column.append(Paragraph("Boundaries", styles['Heading2']))
        
        for i, b in enumerate(boundaries_data):
            boundary_data = [
                [Paragraph(f"<b>North:</b> {b.get('north', 'Not Found')}", styles['SmallNormal'])],
                [Paragraph(f"<b>South:</b> {b.get('south', 'Not Found')}", styles['SmallNormal'])],
                [Paragraph(f"<b>East:</b> {b.get('east', 'Not Found')}", styles['SmallNormal'])],
                [Paragraph(f"<b>West:</b> {b.get('west', 'Not Found')}", styles['SmallNormal'])]
            ]
            boundary_table = Table(boundary_data, colWidths=[5*inch])
            boundary_table.setStyle(TableStyle([
                ('GRID', (0,0), (-1,-1), 0.5, colors.grey),
                ('VALIGN', (0,0), (-1,-1), 'TOP')
            ]))
            left_column.append(boundary_table)
            if i < len(boundaries_data) - 1:
                left_column.append(Spacer(1, 3))
    
    # Title History
    if isinstance(analysis_data.get("TITLE HISTORY (CHRONOLOGICAL ORDER)"), list) and analysis_data["TITLE HISTORY (CHRONOLOGICAL ORDER)"]:
        left_column.append(Spacer(1, 5))
        left_column.append(Paragraph("Title History", styles['Heading2']))
        
        history_data = [[
            Paragraph("<b>Doc</b>", styles['Tiny']),
            Paragraph("<b>Date</b>", styles['Tiny']),
            Paragraph("<b>From</b>", styles['Tiny']),
            Paragraph("<b>To</b>", styles['Tiny']),
            Paragraph("<b>Type</b>", styles['Tiny'])
        ]]
        
        for entry in analysis_data["TITLE HISTORY (CHRONOLOGICAL ORDER)"]:
            row = [
                Paragraph(str(entry.get("document_name", ""))[:15], styles['Tiny']),
                Paragraph(str(entry.get("document_date", ""))[:10], styles['Tiny']),
                Paragraph(str(entry.get("executed_by", ""))[:15], styles['Tiny']),
                Paragraph(str(entry.get("in_favour_of", ""))[:15], styles['Tiny']),
                Paragraph(str(entry.get("nature_of_transfer", ""))[:15], styles['Tiny'])
            ]
            history_data.append(row)
        
        history_table = Table(history_data, colWidths=[1*inch, 0.7*inch, 1.1*inch, 1.1*inch, 1.1*inch])
        history_table.setStyle(TableStyle([
            ('BACKGROUND', (0,0), (-1,0), colors.lightgrey),
            ('GRID', (0,0), (-1,-1), 0.5, colors.black),
            ('VALIGN', (0,0), (-1,-1), 'TOP')
        ]))
        left_column.append(history_table)
    
    # --- RIGHT COLUMN CONTENT ---
    right_column = []
    
    # Legal Opinion
    opinion = analysis_data.get("LEGAL OPINION (INITIAL & DETAILED)", {})
    if isinstance(opinion, dict):
        right_column.append(Paragraph("Initial Legal Opinion (from Document)", styles['Heading2']))
        
        metrics = {
            "title_clear": "Title Status",
            "encumbrances": "Encumbrances",
            "mortgage_viability": "Mortgage Viability"
        }
        
        opinion_data = []
        for key, title in metrics.items():
            metric = opinion.get(key, {})
            if isinstance(metric, dict):
                verdict = metric.get("verdict", "N/A")
                justification = metric.get("detailed_justification", "No justification provided.")
                
                opinion_data.append([
                    Paragraph(f"<b>{title}:</b>", styles['SmallNormal']),
                    Paragraph(f"<b>{verdict}</b>", styles['SmallNormal']),
                    Paragraph(justification[:150] + "..." if len(justification) > 150 else justification,
                             styles['SmallNormal'])
                ])
        
        opinion_table = Table(opinion_data, colWidths=[1.2*inch, 0.8*inch, 3*inch])
        opinion_table.setStyle(TableStyle([
            ('GRID', (0,0), (-1,-1), 0.5, colors.grey),
            ('BACKGROUND', (0,0), (0,-1), colors.lightgrey),
            ('VALIGN', (0,0), (-1,-1), 'TOP')
        ]))
        right_column.append(opinion_table)
        
        if "summarized_opinion" in opinion:
            summary = opinion.get("summarized_opinion", "")
            conclusion_data = [[
                Paragraph("<b>Summarized Opinion:</b>", styles['SmallNormal']),
                Paragraph(summary[:200] + "..." if len(summary) > 200 else summary, styles['SmallNormal'])
            ]]
            conclusion_table = Table(conclusion_data, colWidths=[1.2*inch, 3.8*inch])
            conclusion_table.setStyle(TableStyle([
                ('GRID', (0,0), (-1,-1), 0.5, colors.grey),
                ('VALIGN', (0,0), (-1,-1), 'TOP')
            ]))
            right_column.append(conclusion_table)

    # Potential Issues
    issues = analysis_data.get("POTENTIAL ISSUES (FLAGS)", [])
    if isinstance(issues, list) and issues:
        right_column.append(Spacer(1, 5))
        right_column.append(Paragraph("Potential Issues", styles['Heading2']))
        
        issues_data = [[
            Paragraph("<b>Type</b>", styles['Tiny']),
            Paragraph("<b>Description</b>", styles['Tiny']),
            Paragraph("<b>Severity</b>", styles['Tiny'])
        ]]
        
        for issue in issues:
            row = [
                Paragraph(str(issue.get("issue_type", ""))[:15], styles['Tiny']),
                Paragraph(str(issue.get("description", ""))[:40], styles['Tiny']),
                Paragraph(str(issue.get("severity", "")), styles['Tiny'])
            ]
            issues_data.append(row)
        
        issues_table = Table(issues_data, colWidths=[1.2*inch, 3*inch, 0.8*inch])
        issues_table.setStyle(TableStyle([
            ('BACKGROUND', (0,0), (-1,0), colors.lightgrey),
            ('GRID', (0,0), (-1,-1), 0.5, colors.black),
            ('VALIGN', (0,0), (-1,-1), 'TOP')
        ]))
        right_column.append(issues_table)
    
    # Documents Examined (Full List)
    docs_examined = analysis_data.get("DOCUMENTS EXAMINED", [])
    if isinstance(docs_examined, list) and docs_examined:
        right_column.append(Spacer(1, 5))
        right_column.append(Paragraph("Documents Examined", styles['Heading2']))
        
        docs_data = [[
            Paragraph("<b>Document (Full Description)</b>", styles['Tiny'])
        ]]
        
        for doc in docs_examined:
            row = [Paragraph(str(doc.get("full_description", doc.get("document_name", ""))), styles['Tiny'])]
            docs_data.append(row)
        
        docs_table = Table(docs_data, colWidths=[5*inch])
        docs_table.setStyle(TableStyle([
            ('BACKGROUND', (0,0), (-1,0), colors.lightgrey),
            ('GRID', (0,0), (-1,-1), 0.5, colors.black),
            ('VALIGN', (0,0), (-1,-1), 'TOP')
        ]))
        right_column.append(docs_table)

    data = [[
        Table([[cell] for cell in left_column], colWidths=[5*inch]),
        Table([[cell] for cell in right_column], colWidths=[5*inch])
    ]]
    
    main_table = Table(data, colWidths=[5.2*inch, 5.2*inch])
    main_table.setStyle(TableStyle([
        ('VALIGN', (0,0), (-1,-1), 'TOP'),
        ('LEFTPADDING', (0,0), (-1,-1), 3),
        ('RIGHTPADDING', (0,0), (-1,-1), 3),
        ('TOPPADDING', (0,0), (-1,-1), 3),
        ('BOTTOMPADDING', (0,0), (-1,-1), 3)
    ]))
    elements.append(main_table)
    
    # Build the PDF
    try:
        doc_template.build(elements)
    except Exception as e:
        print(f"PDF generation failed: {e}")
        # Fallback to a simple text report in the buffer if PDF fails
        buffer.seek(0)
        buffer.truncate()
        buffer.write(f"PDF Generation Failed: {e}\n\n".encode('utf-8'))
        buffer.write(json.dumps(analysis_data, indent=2).encode('utf-8'))

    return buffer.getvalue()


def main():
    st.markdown("""
        <style>
            [data-testid="stHeader"] {visibility: hidden; height: 0%; position: fixed;}
            .st-emotion-cache-1avcm0n {height: 100%; display: flex; align-items: center; justify-content: center;}
        </style>""", unsafe_allow_html=True)

    logo_col, title_col = st.columns()
    with logo_col:
        try: st.image('chola.png', width=200)
        except Exception: st.write(" ")

    # Directly render the single document view as it's the only mode
    render_single_doc_view()

    st.markdown("---")
    if st.button("üîÑ Clear All State & Refresh Page"):
        keys_to_clear = list(st.session_state.keys())
        for key in keys_to_clear: del st.session_state[key]
        output_dir_path = Path(OUTPUT_DIR)
        if output_dir_path.exists():
            try:
                shutil.rmtree(output_dir_path)
                ensure_dir(OUTPUT_DIR)
                st.toast(f"Cleared output directory '{OUTPUT_DIR}'.")
            except Exception as e:
                st.warning(f"Could not clear '{OUTPUT_DIR}': {e}.")
        st.rerun()

if __name__ == "__main__":
    main()
``````
